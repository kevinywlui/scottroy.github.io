<!doctype html>
<html>

<head>

  <title>
    
      Autoencoders | statsandstuff
    
  </title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">
  <!-- Use Atom -->
    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="statsandstuff" />
  <!-- Use RSS-2.0 -->
  <!--<link href="/rss-feed.xml" type="application/rss+xml" rel="alternate" title="statsandstuff | a blog on statistics and machine learning"/>
  //-->

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quattrocento+Sans">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>


  <!-- Use Jekyll SEO plugin -->
  <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Autoencoders | statsandstuff</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Autoencoders" />
<meta name="author" content="Scott Roy" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="High dimensional data is usually very structured, and this structure allows us to map the data to a smaller dimensional space without losing much information.  For example, if this post is encoded with ASCII, it lives in a point space.  But since I’m writing in English, the sequence of characters “Life is a tale told by an idiot, full of sound and fury, signifying nothing” is far more likely than the sequence “fjkla;8#knxHJD38lkdf dfjkal.”  Thus my post really lives in a very low-dimensional “English subspace” of the full  point space.  The same principle applies to lots of other data.  For example, strictly speaking a 10 megapixel image is described by about 30 million numbers (about 10 million RGB triples), but since people take pictures of predictable things such as cats, dogs, faces, rocks, etc., you can describe the picture using much fewer than 30 million numbers.  Dimensionality reduction is used for image compression, audio compression, and video compression.  In learning algorithms, we have high dimensional feature vectors (e.g., the features could be the RGB values in an image), but the algorithms behave better if we first map the features to a low-dimensional space." />
<meta property="og:description" content="High dimensional data is usually very structured, and this structure allows us to map the data to a smaller dimensional space without losing much information.  For example, if this post is encoded with ASCII, it lives in a point space.  But since I’m writing in English, the sequence of characters “Life is a tale told by an idiot, full of sound and fury, signifying nothing” is far more likely than the sequence “fjkla;8#knxHJD38lkdf dfjkal.”  Thus my post really lives in a very low-dimensional “English subspace” of the full  point space.  The same principle applies to lots of other data.  For example, strictly speaking a 10 megapixel image is described by about 30 million numbers (about 10 million RGB triples), but since people take pictures of predictable things such as cats, dogs, faces, rocks, etc., you can describe the picture using much fewer than 30 million numbers.  Dimensionality reduction is used for image compression, audio compression, and video compression.  In learning algorithms, we have high dimensional feature vectors (e.g., the features could be the RGB values in an image), but the algorithms behave better if we first map the features to a low-dimensional space." />
<link rel="canonical" href="http://localhost:4000/autoencoders.html" />
<meta property="og:url" content="http://localhost:4000/autoencoders.html" />
<meta property="og:site_name" content="statsandstuff" />
<meta property="og:image" content="http://localhost:4000/autoencoder.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-04-13T00:00:00-07:00" />
<script type="application/ld+json">
{"description":"High dimensional data is usually very structured, and this structure allows us to map the data to a smaller dimensional space without losing much information.  For example, if this post is encoded with ASCII, it lives in a point space.  But since I’m writing in English, the sequence of characters “Life is a tale told by an idiot, full of sound and fury, signifying nothing” is far more likely than the sequence “fjkla;8#knxHJD38lkdf dfjkal.”  Thus my post really lives in a very low-dimensional “English subspace” of the full  point space.  The same principle applies to lots of other data.  For example, strictly speaking a 10 megapixel image is described by about 30 million numbers (about 10 million RGB triples), but since people take pictures of predictable things such as cats, dogs, faces, rocks, etc., you can describe the picture using much fewer than 30 million numbers.  Dimensionality reduction is used for image compression, audio compression, and video compression.  In learning algorithms, we have high dimensional feature vectors (e.g., the features could be the RGB values in an image), but the algorithms behave better if we first map the features to a low-dimensional space.","author":{"@type":"Person","name":"Scott Roy"},"@type":"BlogPosting","url":"http://localhost:4000/autoencoders.html","image":"http://localhost:4000/autoencoder.png","headline":"Autoencoders","dateModified":"2018-04-13T00:00:00-07:00","datePublished":"2018-04-13T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/autoencoders.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


</head>


<body>

  <div class="container">
    <header class="masthead">
  <h3 class="masthead-title">
    <a href="/">statsandstuff</a>
    <small class="masthead-subtitle">a blog on statistics and machine learning</small>
    <div class="menu">
  <nav class="menu-content">
    
      <a href="/menu/about.html">About</a>
    
      <a href="/menu/writing.html">Writing</a>
    
      <a href="/menu/contact.html">Contact</a>
    
  </nav>
  <nav class="social-icons">
    
  
  
    <a href="https://www.github.com/scottroy" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://www.linkedin.com/in/scott-roy/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  

  
  
    <a href="mailto:scott.michael.roy@gmail.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  

  
  
    <a href="/feed.xml"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  

  </nav>
</div>

  </h3>
</header>


    <div class="post-container">
      <h1>
  Autoencoders
</h1>


  <img src="/assets/img/autoencoder.png">


<p>High dimensional data is usually very structured, and this structure allows us to map the data to a smaller dimensional space without losing much information.  For example, if this post is encoded with ASCII, it lives in a <script type="math/tex">(256)^{\text{length post}}</script> point space.  But since I’m writing in English, the sequence of characters “Life is a tale told by an idiot, full of sound and fury, signifying nothing” is far more likely than the sequence “fjkla;8#knxHJD38lkdf dfjkal.”  Thus my post really lives in a very low-dimensional “English subspace” of the full <script type="math/tex">(256)^{\text{length post}}</script> point space.  The same principle applies to lots of other data.  For example, strictly speaking a 10 megapixel image is described by about 30 million numbers (about 10 million RGB triples), but since people take pictures of predictable things such as cats, dogs, faces, rocks, etc., you can describe the picture using much fewer than 30 million numbers.  Dimensionality reduction is used for image compression, audio compression, and video compression.  In learning algorithms, we have high dimensional feature vectors (e.g., the features could be the RGB values in an image), but the algorithms behave better if we first map the features to a low-dimensional space.</p>

<p>The problem is to map <script type="math/tex">N</script> dimensional data to a <script type="math/tex">d</script> dimensional code, and the goal is to learn an encoder <script type="math/tex">f : \mathbb{R}^N \to \mathbb{R}^d</script> and a decoder <script type="math/tex">g : \mathbb{R}^d \to \mathbb{R}^N</script> such that <script type="math/tex">g(f(x)) \approx x</script>.  PCA (principal component analysis) is historically the most popular method of dimensionality reduction.  In PCA, both the encoder and decoder are linear maps and geometrically it’s finding a <script type="math/tex">d</script> dimensional hyperplane that the <script type="math/tex">N</script> dimensional data hovers around.  A more modern approach uses autoencoders.</p>

<p>An autoencoder is a feed-forward neural network that we train so that the <script type="math/tex">N</script> dimensional output layer reconstructs the <script type="math/tex">N</script>-dimensional input layer.  Internally there is a hidden “code” layer with <script type="math/tex">d</script> nodes.  The encoder is the “bottom” part of the network mapping the <script type="math/tex">N</script> dimensional input layer to the <script type="math/tex">d</script> dimensional code layer, and the decoder is the “top” part of the network mapping the <script type="math/tex">d</script> dimensional code layer to the <script type="math/tex">N</script> dimensional output layer.  In training the autoencoder, we minimize</p>

<script type="math/tex; mode=display">L(x, g(f(x))),</script>

<p>where the loss <script type="math/tex">L</script> tries to force the reconstruction <script type="math/tex">g(f(x))</script> to be close the input <script type="math/tex">x</script>.  To prevent the autoencoder from learning the identity, we impose a structural constraint in the network by making <script type="math/tex">d \ll N</script>, or we add regularization to the loss function and try to minimize</p>

<script type="math/tex; mode=display">L(x, g(f(x))) + \Omega(h),</script>

<p>where <script type="math/tex">h = f(x)</script> is the hidden code layer, and <script type="math/tex">\Omega</script> imposes some structure on <script type="math/tex">h</script>, e.g., sparsity.</p>


<span class="post-date">
  Written on
  
  April
  13th,
  2018
  by
  
    Scott Roy
  
</span>

<div class="post-date">Feel free to share!</div>
  <div class="sharing-icons">
    <a href="https://twitter.com/intent/tweet?text=Autoencoders&amp;url=/autoencoders.html" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
    <a href="https://www.facebook.com/sharer/sharer.php?u=/autoencoders.html&amp;title=Autoencoders" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
    <a href="https://plus.google.com/share?url=/autoencoders.html" target="_blank"><i class="fa fa-google-plus" aria-hidden="true"></i></a>
  </div>
</div>


<div class="related">
  <h1 >You may also enjoy:</h1>
  
  <ul class="related-posts">
    
  </ul>
</div>




    </div>

    <footer class="footer">
  
  
  
    <a href="https://www.github.com/scottroy" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://www.linkedin.com/in/scott-roy/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  

  
  
    <a href="mailto:scott.michael.roy@gmail.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  

  
  
    <a href="/feed.xml"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  

  <div class="post-date"><a href="/menu/about.html">statsandstuff | a blog on statistics and machine learning by Scott Roy</a></div>
</footer>

  </div>

</body>
</html>
