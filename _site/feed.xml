<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-03-01T20:03:55-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">statsandstuff</title><subtitle>a blog on statistics and machine learning</subtitle><author><name>Scott Roy</name></author><entry><title type="html">Controlling error when testing many hypotheses</title><link href="http://localhost:4000/controlling-error-when-testing-many-hypotheses.html" rel="alternate" type="text/html" title="Controlling error when testing many hypotheses" /><published>2018-11-18T00:00:00-08:00</published><updated>2018-11-18T00:00:00-08:00</updated><id>http://localhost:4000/controlling-error-when-testing-many-hypotheses</id><content type="html" xml:base="http://localhost:4000/controlling-error-when-testing-many-hypotheses.html">&lt;p&gt;In a hypothesis test, we compute some test statistic &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; that is designed to distinguish between a null and alternative hypothesis.  We then compute the probability p(T) of observing a test statistic as large or more extreme as T under the null hypothesis, and reject the null hypothesis if the p-value p(T) is sufficiently small. (As an aside, the p-value can alternatively be viewed as the probability, under the null hypothesis, of observing data as rare or rarer than the data we actually saw.  This perspective does not require coming up with a test statistic first.)&lt;/p&gt;

&lt;p&gt;When we perform many tests (for example, testing association with disease on thousands of genes), we are likely to get false positives, even if each individual test has a small probability of error (see &lt;a href=&quot;/multiple-hypothesis-tests.html&quot;&gt;Multiple hypothesis tests&lt;/a&gt;).  With careful analysis, though, we can understand (and therefore control) the number of false positives our battery of tests yields.&lt;/p&gt;

&lt;p&gt;The most important insight into analyzing the “multiple testing problem” is the observation that under the null hypothesis, the p-values are uniformly distributed.  To see this, suppose that under the null, the test statistic is distributed &lt;script type=&quot;math/tex&quot;&gt;T \sim f&lt;/script&gt;.  The p-value &lt;script type=&quot;math/tex&quot;&gt;p(T)&lt;/script&gt; is at most &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; if the test statistic &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; falls in the &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;-tail of the distribution &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.  By definition, this happens with probability &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;, and so &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(p(T) \leq \alpha) = \alpha&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p(T) \sim \text{Uniform}(0,1)&lt;/script&gt;. (This is more or less the same reasoning used in the inverse CDF method; see &lt;a href=&quot;/sampling-from-distributions.html&quot;&gt;Sampling from distributions&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;Throughout we assume that we test &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; independent hypotheses, &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; of which are null and &lt;script type=&quot;math/tex&quot;&gt;m_1 = m - m_0&lt;/script&gt; of which are alternative.  We do not know the number of null hypotheses &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; a priori. (If &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; is large, we can estimate it from a p-value histogram because &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; of the p-values are uniformly distributed.)  We reject a hypothesis if its p-value is less than some threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.  We let V denote the number null hypotheses we rejected.&lt;/p&gt;

&lt;h2 id=&quot;bonferroni-correction&quot;&gt;Bonferroni correction&lt;/h2&gt;
&lt;p&gt;The Bonferroni correction sets a rejection threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; so that the probability of having a false positive is controlled at level &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(V \geq 1) \leq \alpha&lt;/script&gt;.  At significance level &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, what is the probability that we reject some null hypothesis?  It is easier to compute the probability of the complement event that we do not reject any null hypotheses.  This happens if &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; i.i.d. uniform p-values land in the interval &lt;script type=&quot;math/tex&quot;&gt;[t,1]&lt;/script&gt;, which occurs with probability &lt;script type=&quot;math/tex&quot;&gt;(1-t)^{m_0}&lt;/script&gt;.  We thus have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\textbf{P}(V \geq 1) = 1 - (1-t)^{m_0} \leq \alpha \quad &amp;\Leftrightarrow \quad t \leq 1 - (1-\alpha)^{1/m_0}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The threshold &lt;script type=&quot;math/tex&quot;&gt;t(\alpha) = 1 - (1-\alpha)^{1/m_0}&lt;/script&gt; caps the family-wise error rate (FWER) &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(V \geq 1)&lt;/script&gt; at &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;.  We do not know &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; a priori, but replacing &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; only makes the threshold smaller, and therefore still controls the FWER.  If we have a better upper estimate of &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; (from a histogram of p-values, for example), we can use it instead of &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The formula for the threshold &lt;script type=&quot;math/tex&quot;&gt;t(\alpha)&lt;/script&gt; is quite ugly, but is convex and can therefore be underestimated with its tangent line &lt;script type=&quot;math/tex&quot;&gt;l(\alpha) = \alpha/m_0&lt;/script&gt;  at &lt;script type=&quot;math/tex&quot;&gt;\alpha=0&lt;/script&gt;.  The approximation is near perfect for practical values of &lt;script type=&quot;math/tex&quot;&gt;\alpha \leq 0.1&lt;/script&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/FWER.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/FWER2.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The Bonferroni correction controls the FWER at &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; and then tests for significance at level &lt;script type=&quot;math/tex&quot;&gt;\alpha/m&lt;/script&gt; (it is usually proved in a simpler way using a union bound).  For a large numbers of tests, the Bonferroni correction is too strict and often results in no positive findings.&lt;/p&gt;

&lt;p&gt;A slight improvement that still controls the FWER at level &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is the &lt;strong&gt;Holm-Bonferroni method&lt;/strong&gt;.  This procedure gradually increases the significance level.  The smallest p-value &lt;script type=&quot;math/tex&quot;&gt;p_{(1)}&lt;/script&gt; is tested at threshold &lt;script type=&quot;math/tex&quot;&gt;\alpha / m&lt;/script&gt; (the same as the Bonferroni method), but the next smallest p-value &lt;script type=&quot;math/tex&quot;&gt;p_{(2)}&lt;/script&gt; is tested with threshold &lt;script type=&quot;math/tex&quot;&gt;\alpha / (m-1)&lt;/script&gt;, and the next smallest is tested at level &lt;script type=&quot;math/tex&quot;&gt;\alpha / (m-2)&lt;/script&gt;, and so on.  The procedure stops when a p-value is not rejected.  The Holm-Bonferroni method is still very strict when &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; is large.  (Both the Bonferroni method and the Holm-Bonferroni procedure can be made more powerful by replacing &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; with a better overestimate of &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt;.)&lt;/p&gt;

&lt;p&gt;The methods we discuss next allow some false positives, but control the number of false positives.&lt;/p&gt;

&lt;h2 id=&quot;controlling-k-fwer&quot;&gt;Controlling k-FWER&lt;/h2&gt;
&lt;p&gt;Suppose we’re willing to allow &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; false positives, but control the probability &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(V \geq k+1)&lt;/script&gt; of having more than &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; false positives (called the k-FWER).  Notice that we have more than &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; false positives if the &lt;script type=&quot;math/tex&quot;&gt;(k+1)&lt;/script&gt;th largest null p-value is less than the significance threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.  Since the null p-values are uniformly distributed, the &lt;script type=&quot;math/tex&quot;&gt;(k+1)&lt;/script&gt;th largest null p-value is distributed &lt;script type=&quot;math/tex&quot;&gt;\text{Beta}(k+1, m_0 - k)&lt;/script&gt;.  We simply find the threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}( \text{Beta}(k+1, m_0 - k) \leq t) = \alpha&lt;/script&gt;.  Here is a plot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/kFWER.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;controlling-the-false-discovery-rate&quot;&gt;Controlling the false discovery rate&lt;/h2&gt;
&lt;p&gt;Roughly speaking, the false discovery rate (FDR) is &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(\text{test is null} \vert  \text{test is rejected})&lt;/script&gt;.  This is the reverse of the false positive rate &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(\text{test is rejected} \vert  \text{test is null})&lt;/script&gt;, the quantity that is traditionally controlled in hypothesis testing.  Limiting the FDR and the FPR controls the number of false positives, but the denominators used to compute the two rates differ.  The FDR uses the number of rejections in the denominator, whereas the the FPR uses the number of null tests.  The difference between the two is much like the difference between precision and recall.&lt;/p&gt;

&lt;p&gt;Below is the p-value histogram for 10,000 t-tests for a difference in two means.  The two means were equal in about 70% of the tests (70% of the tests were null).  The p-value distribution is a mixture of a uniform distribution (from the null tests) and a distribution concentrated near 0 (from the non-null tests).  A priori we do not know which p-values correspond to the null tests (we observe the black histogram on the left); since this data is simulated, though, I show which p-values correspond to null tests in the red/blue histogram on the right.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/pvalue_hist_black.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/pvalue_hist.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The FDR at significance threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is the number of null p-values to the left of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; over the total number of p-values to the left of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; (the proportion of blue area to the left of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; in the histogram.)  In most cases, the FDR decreases with the significance threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.  Below we zoom in on the histogram in the &lt;script type=&quot;math/tex&quot;&gt;[0, 0.05]&lt;/script&gt; region.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/pvalue_hist2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A priori we do not know which p-values correspond to null tests (the blue portion of the p-value histogram).  Nonetheless we can assume that all p-values bigger than, for example, 0.5 correspond to null tests.  In this case, we estimate the number of null tests via the relation &lt;script type=&quot;math/tex&quot;&gt;0.5 m_0 = \# \{ \text{p-values} \geq 0.5\}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; is the (unknown) number of null tests.  Rather than use 0.5, we can parametrize with &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; and estimate the fraction of null tests &lt;script type=&quot;math/tex&quot;&gt;\frac{m_0}{m}&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}_0(s) = \frac{\# \{ \text{p-values} \geq s\}}{s m}&lt;/script&gt;.  The estimate is best (but noisy) for values of &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; near 1 (&lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; controls the bias-variance tradeoff).  Here is a plot of &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}_0(s)&lt;/script&gt; versus &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/pi0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Storey and Tibshirani suggest fitting a weighted cubic spline to the curve &lt;script type=&quot;math/tex&quot;&gt;s \mapsto \hat{\pi}_0(s)&lt;/script&gt; and evaluating the spline at 1.&lt;/p&gt;

&lt;p&gt;The estimated FDR at threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{FDR}(t) &amp;= \frac{m \hat{\pi}_0 t}{\# \{ \text{p-values} \leq t\}}. \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;We plot this over &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/fdr.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/fdr2.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Suppose we set &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; to the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;th largest p-value &lt;script type=&quot;math/tex&quot;&gt;p_{(k)}&lt;/script&gt; so that we reject the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; smallest p-values.  Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{FDR}(p_{(k)}) = \frac{m \hat{\pi}_0 p_{(k)}}{k}.&lt;/script&gt;

&lt;p&gt;The q-value &lt;script type=&quot;math/tex&quot;&gt;q_{(k)}&lt;/script&gt; corresponding to the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;th largest p-value &lt;script type=&quot;math/tex&quot;&gt;p_{(k)}&lt;/script&gt; is the smallest FDR you can get if you reject the first &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; p-values.  In other words, it is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} q_{(k)} &amp;= \min_{j=k}^m \text{FDR}(p_{(j)}) \\ &amp;= \min_{j=k}^m \frac{m \hat{\pi}_0 p_{(j)}}{j} \\ &amp;= \min \left( \frac{m \hat{\pi}_0 p_{(k)}}{k},\ q_{(k+1)} \right),
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;q_{(m+1)} = \infty&lt;/script&gt;.  To control the FDR at level &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;, we reject all hypotheses with q-value at most &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;.  (The q-value gets its name from the fact that the letter q is a reflection of p and, roughly speaking, the q-value is &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(\text{test is null} \vert  \text{test is rejected})&lt;/script&gt; and the p-value is &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(\text{test is rejected} \vert  \text{test is null})&lt;/script&gt;.)&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Benjamini-Hochberg procedure&lt;/strong&gt; uses the same “q-values,” (discussed in Multiple hypothesis tests) but with the crude estimate &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}_0 = 1&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Statistical Significance for Genome-Wide Experiments&lt;/em&gt; by Storey and Tibshirani&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;The positive false discovery rate: A Bayesian interpretation and the q-value&lt;/em&gt; by Storey&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Scott Roy</name></author><category term="Benjamini-Hochberg" /><category term="Bonferroni correction" /><category term="FWER" /><category term="Holm-Bonferroni" /><category term="k-FWER" /><category term="multiple hypothesis tests" /><category term="p-value" /><category term="q-value" /><category term="false discovery rate" /><category term="type I error" /><category term="type II error" /><summary type="html">In a hypothesis test, we compute some test statistic that is designed to distinguish between a null and alternative hypothesis.  We then compute the probability p(T) of observing a test statistic as large or more extreme as T under the null hypothesis, and reject the null hypothesis if the p-value p(T) is sufficiently small. (As an aside, the p-value can alternatively be viewed as the probability, under the null hypothesis, of observing data as rare or rarer than the data we actually saw.  This perspective does not require coming up with a test statistic first.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/pvalue_hist.png" /></entry><entry><title type="html">Calibration in logistic regression and other generalized linear models</title><link href="http://localhost:4000/calibration-in-logistic-regression-and-other-generalized-linear-models.html" rel="alternate" type="text/html" title="Calibration in logistic regression and other generalized linear models" /><published>2018-08-21T00:00:00-07:00</published><updated>2018-08-21T00:00:00-07:00</updated><id>http://localhost:4000/calibration-in-logistic-regression-and-other-generalized-linear-models</id><content type="html" xml:base="http://localhost:4000/calibration-in-logistic-regression-and-other-generalized-linear-models.html">&lt;p&gt;In general, scores returned by machine learning models are not necessarily well-calibrated probabilities (see my post on &lt;a href=&quot;/ROC-space-and-AUC.html&quot;&gt;ROC space and AUC&lt;/a&gt;).  The probability estimates from a logistic regression model (without regularization) are partially calibrated, though.  In fact, many generalized linear models, including linear regression, logistic regression, binomial regression, and Poisson regression, give calibrated predicted values.&lt;/p&gt;

&lt;h2 id=&quot;where-calibrated-models-are-important&quot;&gt;Where calibrated models are important?&lt;/h2&gt;
&lt;p&gt;Before delving deeper into why most generalized linear models give calibrated estimates, let’s consider a situation in which calibrated estimates are important.  In online advertising, such as on Google or Facebook, an advertiser pays the ad company only when a user clicks on an ad (they are not charged just to show the ad).  An important task for the ad company is to decide which ads to show in its limited ad space.  Simply showing the ad with the highest bid will not maximize the ad company’s revenue.  For example, suppose that advertiser A has bid $10 for every click and advertiser B has bid $1 for every click.  Although advertiser B has bid less, suppose its ad is 20 times more likely to be clicked on.  In this case, the ad company will make twice as much money showing advertiser B’s ad (even though advertiser A has bid 10 times as much per click).  When deciding which ads to show, the ad company must consider two factors: 1) how much the advertiser has bid to pay the ad company each time its ad is clicked and 2) how likely a user is to click on the ad.  Inaccurately predicting how likely a user is to click on an ad may cause the ad company to make a suboptimal decision in which ad to show.  Logistic regression, discussed next, is very popular in online advertising.&lt;/p&gt;

&lt;h2 id=&quot;logistic-regression&quot;&gt;Logistic regression&lt;/h2&gt;
&lt;p&gt;In the logistic regression model, each unit of observation &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has a binary response &lt;script type=&quot;math/tex&quot;&gt;y_i \in \{ 0, 1\}&lt;/script&gt;, where the probability &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; that &lt;script type=&quot;math/tex&quot;&gt;y_i = 1&lt;/script&gt; depends on some features &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; of the unit.  The units are assumed independent, but they are not i.i.d. since the probability that the response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is 1 varies for each unit, depending on its features &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt;.  The units are linked by assuming that &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; has a specific parametric form, with shared parameters &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; across all units.  In particular, the log-odds &lt;script type=&quot;math/tex&quot;&gt;\log \left( \frac{p_i}{1-p_i} \right)&lt;/script&gt; is assumed a linear function of the predictors with coefficients &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log \left( \frac{p_i}{1-p_i} \right) = \beta^T X_i = \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots \beta_p X_{ip}.&lt;/script&gt;

&lt;p&gt;The log-odds function is also called the logit function &lt;script type=&quot;math/tex&quot;&gt;\text{logit}(p) = \log \left( \frac{p}{1-p} \right)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;By inverting the logit, we get the parametric form for the probabilities: &lt;script type=&quot;math/tex&quot;&gt;p_i = \text{logit}^{-1}(p_i) = \frac{1}{1 + e^{-\beta^T X_i}}&lt;/script&gt;.  The inverse of the logit is called the logistic function (logistic regression is so-named because it models probabilities with a logistic function).  The estimates in logistic regression are harder to interpret than those in linear regression because increasing a predictor by 1 does not change the probability of outcome by a fixed amount.  This is because the logistic function &lt;script type=&quot;math/tex&quot;&gt;p(t) = \frac{1}{1 + e^{-t}}&lt;/script&gt; is not a straight line (see the graph below).  Nevertheless, the logistic is nearly linear for values of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; between -1 and 1, which corresponds to probabilities between 0.27 and 0.73 (see dashed red line in figure).  The slope of the dashed red line is 1/4 (the derivative of the logistic at &lt;script type=&quot;math/tex&quot;&gt;t = 0&lt;/script&gt;).  For a moderate range of probabilities (about 0.3 to 0.7), increasing the covariate &lt;script type=&quot;math/tex&quot;&gt;X_{ij}&lt;/script&gt; by 1 will change the predicted probability by about &lt;script type=&quot;math/tex&quot;&gt;\frac{\beta_j}{4}&lt;/script&gt; (increase or decrease, depending on the sign of &lt;script type=&quot;math/tex&quot;&gt;\beta_j&lt;/script&gt;).  Since the red line is the steepest part of the logistic curve, the approximated change &lt;script type=&quot;math/tex&quot;&gt;\frac{\beta_j}{4}&lt;/script&gt; is always an upper bound (even for probabilities &lt;strong&gt;outside&lt;/strong&gt; the range 0.3 to 0.7).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/logistic.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;fitting-logistic-regression-and-calibration&quot;&gt;Fitting logistic regression and calibration&lt;/h2&gt;
&lt;p&gt;Logistic regression is fit with maximum likelihood estimation.  The likelihood is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\beta) = \prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i}&lt;/script&gt;

&lt;p&gt;and the negative log-likelihood is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\beta) = \sum_{i=1}^n -y_i \log p_i - (1-y_i) \log (1 - p_i).&lt;/script&gt;

&lt;p&gt;Taking a derivative with respect to &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; (using the fact that &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\beta}\ -\log p_i = -(1-p_i) X_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\beta} -\log(1-p_i) = p_i X_i&lt;/script&gt;), we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla l(\beta) = \sum_{i=1}^n -(1-p_i) y_i X_i + p_i (1-y_i) X_i = \sum_{i=1}^n -y_i X_i + p_i X_i.&lt;/script&gt;

&lt;p&gt;The probabilities thus satisfy&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n y_i X_i = \sum_{i=1}^n p_i X_i.&lt;/script&gt;

&lt;p&gt;These are &lt;strong&gt;calibration equations&lt;/strong&gt;.  They hold for each component of the covariate vector &lt;script type=&quot;math/tex&quot;&gt;X_i = (X_{i1}, X_{i2}, \ldots, X_{ip})&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n y_i X_{ij} = \sum_{i=1}^n p_i X_{ij}.&lt;/script&gt;

&lt;p&gt;Under the logistic model, &lt;script type=&quot;math/tex&quot;&gt;p_i = \text{E}(y_i)&lt;/script&gt; and so the above equations say that the observed value of &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n y_i X_{ij}&lt;/script&gt; in the data equals its expected value, according to the MLE fitted model.  Since &lt;script type=&quot;math/tex&quot;&gt;y_i \in \{0, 1\}&lt;/script&gt;, these equations further simplify to: the observed sum of a covariate over the positive class equals the expected sum of the covariate over the positive class.&lt;/p&gt;

&lt;p&gt;To explain how these equations calibrate the model, let’s walk through an example.  Suppose we are predicting whether an English major is a man or women using 3 predictors: an intercept, an indicator for whether the student likes Jane Austen, and height.  Let &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; be the probability that student &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is a man.  The calibration equations say:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;(Intercept equation) The number of male English majors in the data equals &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n p_i&lt;/script&gt;, the expected number of male English majors in the data, as predicted by the logistic model.&lt;/li&gt;
  &lt;li&gt;(Jane Austen equation) The number of male English majors who like Jane Austen in the data equals &lt;script type=&quot;math/tex&quot;&gt;\sum_{i \text{ likes Jane Austen}} p_i&lt;/script&gt;, the expected number of male English majors who like Jane Austen in the data, as predicted by the logistic model.&lt;/li&gt;
  &lt;li&gt;(Height equation) The sum of heights of all men in the data equals &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n \text{height}_i \cdot p_i&lt;/script&gt;, the expected sum of heights of all men in the data, as predicted by the model.  Combined with the first equation, we could reword this as: the average height of a man in the data equals the expected height of a man, as predicted by the model.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the calibration equations have many solutions for the probabilities.  Logistic regression chooses the solution of the form &lt;script type=&quot;math/tex&quot;&gt;p_i = \frac{1}{1 + e^{-\beta^T X_i}}&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;updown-sampling-will-ruin-logistic-regression-probability-estimates&quot;&gt;Up/down sampling will ruin logistic regression probability estimates&lt;/h2&gt;
&lt;p&gt;The common practice of up/down sampling in machine learning to get class balance will ruin the calibration in logistic regression probability estimates.  To explain, we continue with our previous example (but drop the height covariate).  Consider trying to model whether a student studying English literature is a man or woman, based on whether they like Jane Austen or not.  Suppose 80% of students studying English are women and only 20% are men.  Further suppose that 70% of the women students like Jane Austen, but only 40% of the men do.  Using Bayes’ rule, we can compute&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{P}(\text{man} \vert  \text{likes Jane Austen}) = 12.5\%&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{P}( \text{man} \vert  \text{does not like Jane Austen}) =33.3\%.&lt;/script&gt;

&lt;p&gt;A logistic regression with intercept term and an indicator for whether the English student likes Jane Austen will learn these probabilities (there is a unique solution to the calibration equations in this case).&lt;/p&gt;

&lt;p&gt;The probabilities above depend on the ratio of women to men among English majors.  If instead of 4 women to every man, the ratio is &lt;script type=&quot;math/tex&quot;&gt;\text{P}(\text{woman}) /\text{P}(\text{man})&lt;/script&gt;, the conditional probabilities are&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{P}(\text{man} \vert  \text{likes Jane Austen}) = \frac{0.4}{0.4 + 0.7(\text{P}(\text{woman}) / \text{P}(\text{man}) )}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{P}(\text{man} \vert  \text{does not like Jane Austen}) = \frac{0.6}{0.6 + 0.3(\text{P}(\text{woman}) / \text{P}(\text{man}) )}.&lt;/script&gt;

&lt;p&gt;These are the probabilities that a logistic regression will predict when trained on data where the ratio of women to men is &lt;script type=&quot;math/tex&quot;&gt;\text{P}(\text{woman}) / \text{P}(\text{man})&lt;/script&gt;.  In particular, if we were to artificially balance the classes by downsampling the number of women by 4, logistic regression would return the estimates&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{P}(\text{man} \vert  \text{likes Jane Austen}) = \frac{0.4}{0.4 + 0.7} = 36.4\%&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{P}(\text{man} \vert  \text{does not like Jane Austen}) = \frac{0.6}{0.6 + 0.3} = 66.7\%.&lt;/script&gt;

&lt;p&gt;These probabilities do not match the probabilities &lt;script type=&quot;math/tex&quot;&gt;12.5\%&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;33.3\%&lt;/script&gt; that we observe in reality!  In summary, logistic regression automatically calibrates (to some extent) its predicted probabilities, but this requires that the class balance in the training data match the class balance in the actual data.&lt;/p&gt;

&lt;p&gt;In this rest of this post, I want to go over where else the calibration equations above show up.&lt;/p&gt;

&lt;h2 id=&quot;binomial-regression&quot;&gt;Binomial regression&lt;/h2&gt;
&lt;p&gt;Binomial regression is a generalization of logistic regression.  In binomial regression, each response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is the number of successes in &lt;script type=&quot;math/tex&quot;&gt;n_i&lt;/script&gt; trials, where the probability of success is &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; is modeled with the logistic function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_i = \frac{1}{1 + e^{-\beta^T X_i}}.&lt;/script&gt;

&lt;p&gt;The only change from logistic regression is that the likelihood (up to a constant factor independent of &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;) is now :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\beta) \propto \prod_{i=1}^n p_i^{y_i} (1-p_i)^{n_i-y_i}.&lt;/script&gt;

&lt;p&gt;Working through the derivatives, the MLE estimates for &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; satisfy:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n y_i X_i = \sum_{i=1}^n n_i p_i X_i.&lt;/script&gt;

&lt;p&gt;Notice that &lt;script type=&quot;math/tex&quot;&gt;n_i p_i&lt;/script&gt; is the expected value of &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; under the model.  These are the same calibration equations from logistic regression.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; logistic regression is a special case of binomial regression where &lt;script type=&quot;math/tex&quot;&gt;n_i = 1&lt;/script&gt; for all units.  Similarly, binomial regression is equivalent to a logistic regression where the response &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; and the predictor &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; is repeated &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; times in the data matrix, and the response &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; and the predictor &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; is repeated &lt;script type=&quot;math/tex&quot;&gt;n_i - y_i&lt;/script&gt; times.&lt;/p&gt;

&lt;h2 id=&quot;linear-regression&quot;&gt;Linear regression&lt;/h2&gt;
&lt;p&gt;The regression equations are &lt;script type=&quot;math/tex&quot;&gt;X^T X \beta = X^T Y&lt;/script&gt; (see &lt;a href=&quot;/geometric-interpretations-of-linear-regression-and-ANOVA.html&quot;&gt;Geometric interpretations of linear regression and ANOVA&lt;/a&gt; for more about the geometry behind these equations).  The predicted value in regression is &lt;script type=&quot;math/tex&quot;&gt;\hat{Y} = X \hat{\beta}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt; solves the regression equations.  Thus, the regression equations say that &lt;script type=&quot;math/tex&quot;&gt;X^T \hat{Y} = X^T Y&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n \hat{y}_i X_i = \sum_{i=1}^n y_i X_i&lt;/script&gt;.  Again, these are the calibration equations from above.  Note that &lt;script type=&quot;math/tex&quot;&gt;\hat{y}_i&lt;/script&gt; is the mean of &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; under the linear regression model.&lt;/p&gt;

&lt;h2 id=&quot;poisson-regression&quot;&gt;Poisson regression&lt;/h2&gt;
&lt;p&gt;In Poission regression, the response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is a Poisson random variable with rate &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; is also the mean and variance).  The rates across different units are linked by assuming that the log-rate is a linear function of the predictors &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; with common slope &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;\log \lambda_i = \beta^T X_i&lt;/script&gt;.  Often Poisson regression includes an exposure term &lt;script type=&quot;math/tex&quot;&gt;u_i&lt;/script&gt; so that &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; is the rate per unit of exposure.  In other words, unit &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has response that is modeled Poisson with rate &lt;script type=&quot;math/tex&quot;&gt;u_i \lambda_i&lt;/script&gt;.  The log-rate is &lt;script type=&quot;math/tex&quot;&gt;\log(u_i) + \log(\lambda_i) = \log(u_i) + \beta^T X_i&lt;/script&gt;.  The exposure term &lt;script type=&quot;math/tex&quot;&gt;\log(u_i)&lt;/script&gt; is called the offset and is constrained to have coefficient &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; in the fitting process.&lt;/p&gt;

&lt;p&gt;In Poisson regression, the likelihood is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\beta) = \prod_{i=1}^n \frac{e^{-\lambda_i} \lambda_i^{y_i}}{y_i!},&lt;/script&gt;

&lt;p&gt;and the negative log-likelihood (up to a constant) is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\beta) =  \sum_{i=1}^n \lambda_i - y_i \log \lambda_i.&lt;/script&gt;

&lt;p&gt;Differentiating with respect to &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;, we see that the fitted rates satisfy the calibration equations:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n \lambda_i X_i = \sum_{i=1}^n y_i X_i&lt;/script&gt;

&lt;h2 id=&quot;exponential-family-with-canonical-link-function&quot;&gt;Exponential family with canonical link function&lt;/h2&gt;
&lt;p&gt;The calibration equations hold for any generalized linear model with “canonical” link function.  A random variable &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; follows follows a scalar exponential family distribution if its density is of the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y) = a(\theta) b(y) e^{\theta y},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;a(\theta) &gt; 0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b(y) \geq 0&lt;/script&gt;.  In other words, the parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; only occur together as a product in an exponential.  The mean of an exponential family random variable can be expressed in terms of &lt;script type=&quot;math/tex&quot;&gt;a(\theta)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(y) = -\frac{a'(\theta)}{a(\theta)} = - \frac{\text{d}}{\text{d} \theta} \log a(\theta).&lt;/script&gt;

&lt;p&gt;To see this, differentiate the density in &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \frac{\text{d}}{\text{d} \theta} \ f(y) &amp;=  a'(\theta) b(y) e^{\theta y} + a(\theta) b(y) e^{\theta y} y \\ &amp;= \frac{a'(\theta)}{a(\theta)} f(y) + y f(y) \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and then integrate over &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; (or sum if &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is discrete):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{y} \frac{\text{d}}{\text{d} \theta}\ f(y) = \frac{a'(\theta)}{a(\theta)} + \text{E} \left( y \right).&lt;/script&gt;

&lt;p&gt;By interchanging the derivative and the integral, we see that this quantity is also 0:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{y} \frac{\text{d}}{\text{d} \theta}\ f(y) = \frac{\text{d}}{\text{d} \theta} \int_{y} f(y) = \frac{\text{d}}{\text{d} \theta} 1 = 0.&lt;/script&gt;

&lt;p&gt;To make the concept of an exponential family more concrete, let’s see why the binomial distribution (with fixed number of trials &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;) is an exponential family:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\binom{n}{y} p^y (1-p)^{n-y} = \binom{n}{y} (1-p)^n e^{\log \left( \frac{p}{1-p} \right) \cdot y }.&lt;/script&gt;

&lt;p&gt;In this case, the natural parameter is &lt;script type=&quot;math/tex&quot;&gt;\theta = \log \left( \frac{p}{1-p} \right)&lt;/script&gt;.  The binomial distribution is not an exponential family random variable if the number of trials &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is considered a parameter (because then the range of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; depends on the parameter &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;, which means that &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; are coupled outside the exponential).&lt;/p&gt;

&lt;p&gt;Suppose that the response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; of unit &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has exponential family distribution with natural parameter &lt;script type=&quot;math/tex&quot;&gt;\theta_i&lt;/script&gt;.  Suppose further that the parameters are related by &lt;script type=&quot;math/tex&quot;&gt;\theta_i = \beta^T X_i&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; is a covariate vector for unit &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The negative log-likelihood (up to a constant in &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;) is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\beta) = \sum_{i=1}^n -\log a(\theta_i) - \theta_i y_i.&lt;/script&gt;

&lt;p&gt;Differentiating in &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; (to find the MLE), we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n \text{E} \left( y_i \right) X_i - y_i X_i.&lt;/script&gt;

&lt;p&gt;The expected values &lt;script type=&quot;math/tex&quot;&gt;\text{E}(y_i)&lt;/script&gt; from the MLE fitted model therefore satisfy the calibration equations:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n \text{E} \left( y_i \right) X_i = \sum_{i=1}^n y_i X_i.&lt;/script&gt;</content><author><name>Scott Roy</name></author><category term="calibration" /><category term="exponential family" /><category term="generalized linear model" /><category term="logistic regression" /><summary type="html">In general, scores returned by machine learning models are not necessarily well-calibrated probabilities (see my post on ROC space and AUC).  The probability estimates from a logistic regression model (without regularization) are partially calibrated, though.  In fact, many generalized linear models, including linear regression, logistic regression, binomial regression, and Poisson regression, give calibrated predicted values.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/calibrate.jpg" /></entry><entry><title type="html">Geometric interpretations of linear regression and ANOVA</title><link href="http://localhost:4000/geometric-interpretations-of-linear-regression-and-ANOVA.html" rel="alternate" type="text/html" title="Geometric interpretations of linear regression and ANOVA" /><published>2018-08-05T00:00:00-07:00</published><updated>2018-08-05T00:00:00-07:00</updated><id>http://localhost:4000/geometric-interpretations-of-linear-regression-and-ANOVA</id><content type="html" xml:base="http://localhost:4000/geometric-interpretations-of-linear-regression-and-ANOVA.html">&lt;p&gt;In this post, I explore the connection of linear regression to geometry.  In particular, I discuss the geometric meaning of fitted values, residuals, and degrees of freedom.  Using geometry, I derive coefficient interpretations and discuss omitted variable bias.  I finish by connecting ANOVA (both hypothesis testing and power analysis) to the underlying geometry.&lt;/p&gt;

&lt;h2 id=&quot;fitted-values-are-projections&quot;&gt;Fitted values are projections&lt;/h2&gt;
&lt;p&gt;The fundamental geometric insight is that the predicted values &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}&lt;/script&gt; in a linear regression are the projection of the response &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the linear span of the covariates &lt;script type=&quot;math/tex&quot;&gt;X_0, X_1, \ldots, X_n&lt;/script&gt;.  I’ll call this the &lt;strong&gt;covariate space&lt;/strong&gt;.  The residuals &lt;script type=&quot;math/tex&quot;&gt;Y - \hat{Y}&lt;/script&gt; are therefore the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the orthogonal complement of the covariate space.  I’ll call this the &lt;strong&gt;residual space&lt;/strong&gt;. The residual space contains the part of the data that is unexplained by the model.  To summarize, we can break &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; into two orthogonal pieces: a component in the covariate space (the fitted values from the regression of &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots + X_n&lt;/script&gt;) and a component in the residual space (the residuals of the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots + X_n&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;The fitted values are an orthogonal projection onto the covariate space because the two-norm between &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}&lt;/script&gt; is minimized in the fitting process (the two-norm distance from a point to a surface is minimized when the point is orthogonal to the surface).  I’ll also note that the linear regression equations are the same as the projection equations from linear algebra.  In regression, the parameter &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt; satisfies &lt;script type=&quot;math/tex&quot;&gt;X^T X \hat{\beta} = X^T Y&lt;/script&gt; and so &lt;script type=&quot;math/tex&quot;&gt;\hat{Y} = X \hat{\beta} = X (X^T X)^{-1} X^T Y&lt;/script&gt;.  From linear algebra, &lt;script type=&quot;math/tex&quot;&gt;P = X (X^T X)^{-1} X^T&lt;/script&gt; is the matrix that projects onto the column space of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;.  The connection to orthogonal projections is because of the two-norm: robust regression using a Huber penalty or lasso regression using a one-norm penalty do not have the same geometric interpretation.&lt;/p&gt;

&lt;h2 id=&quot;the-geometry-of-nested-models&quot;&gt;The geometry of nested models&lt;/h2&gt;
&lt;p&gt;Consider a nested model: a small model and a big model, with the covariate space &lt;script type=&quot;math/tex&quot;&gt;L_{\text{small}}&lt;/script&gt; of the small model contained in the covariate space &lt;script type=&quot;math/tex&quot;&gt;L_{\text{big}}&lt;/script&gt; of the big model.  For example, the small model might be the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + X_1&lt;/script&gt; and the big model might be the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + X_1 + X_2&lt;/script&gt;.  Define the delta covariate space &lt;script type=&quot;math/tex&quot;&gt;L_{\text{delta}}&lt;/script&gt; to be the orthogonal complement of &lt;script type=&quot;math/tex&quot;&gt;L_{\text{small}}&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;L_{\text{big}}&lt;/script&gt;.  The picture below shows the small model covariate space, the delta covariate space (orthogonal to the small model covariate space), the big model covariate space (composed of the small model covariate space and the delta covariate space), and the residual space (orthogonal to everything).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/linreg_geometry.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From properties of orthogonal projections, it is clear the fitted values (aka projections of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;) in the small and big model are related by &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{big}} = \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}}&lt;/script&gt;.  This simple geometric equation 1) implies that one-dimensional linear regression is sufficient when covariates are orthogonal, 2) shows that the coefficient on (for example) &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; in the multivariate linear regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots X_n&lt;/script&gt; is the effect of &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; on &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; after controlling for the other covariates, and 3) quantifies omitted variable bias.&lt;/p&gt;

&lt;h2 id=&quot;coefficient-interpretation-and-omitted-variable-bias&quot;&gt;Coefficient interpretation and omitted variable bias&lt;/h2&gt;
&lt;p&gt;Consider the small model &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots X_{n-1}&lt;/script&gt; and the large model &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + X_2 + \ldots X_n&lt;/script&gt;, which has one additional covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt;.  From the geometric relation &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{big}} = \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}}&lt;/script&gt;, we’ll derive coefficient interpretation and omitted variable bias.  Write the fitted values from the small model as &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{small}} = s_0 X_0 + \ldots s_{n-1} X_{n-1}&lt;/script&gt;, where the coefficients &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; are from the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim  X_0 + \ldots X_{n-1}&lt;/script&gt;.  We consider two cases: one where the added covariate is orthogonal to the previous covariates and one where it is not.&lt;/p&gt;

&lt;h3 id=&quot;added-covariate-x_n-is-orthogonal-to-previous-covariates&quot;&gt;Added covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; is orthogonal to previous covariates &lt;/h3&gt;

&lt;p&gt;If the added covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; is orthogonal to the previous covariates &lt;script type=&quot;math/tex&quot;&gt;X_0, \ldots, X_{n-1}&lt;/script&gt;, then the delta covariate space is the line spanned by &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; (i.e., the delta covariate space space is simply the space spanned by the additional covariate).  In this case, &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{delta}} = b_n X_n&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; is the coefficient from the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_n&lt;/script&gt;.  (&lt;script type=&quot;math/tex&quot;&gt;b_n = \frac{Y \cdot X_n}{ X_n \cdot X_n}&lt;/script&gt;.)  Thus &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{big}} = \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}} =s_0 X_0 + \ldots s_{n-1} X_{n-1} + b_n X_n&lt;/script&gt;.  The coefficients for the big regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots + X_n&lt;/script&gt; are &lt;script type=&quot;math/tex&quot;&gt;b_0 = s_0, b_1 = s_1, \ldots, b_{n-1}=s_{n-1}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt;.  In this case, the coefficients in the big model are uncoupled in that the coefficients corresponding to small model covariates can be computed separately from the coefficient on the new covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;In general, orthogonal groups of coefficients are uncoupled and can be handled separately in regression.  In the special case where all covariates are pairwise orthogonal, the coefficients in the big model can be computed by running &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; simple linear regressions &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Finally, I want to discuss what happens when the regression includes an intercept term.  In this case, “orthogonal” is replaced by “uncorrelated.”  Groups of uncorrelated variables can be handled separately, and if all covariates in a multivariate linear regression are pairwise uncorrelated, each coefficient can be computed as the slope in a simple linear regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim 1 + X_i&lt;/script&gt;.  To see why, consider the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the covariate space spanned by &lt;script type=&quot;math/tex&quot;&gt;1, X_1, \ldots, X_n&lt;/script&gt;, where the covariates &lt;script type=&quot;math/tex&quot;&gt;X_1, \ldots, X_n&lt;/script&gt; are pairwise uncorrelated.  The covariate space doesn’t change when we center each covariate by subtracting off its mean (i.e., its projection onto 1).  Uncorrelated means the centered covariates are pairwise orthogonal, and each centered covariate is orthogonal to 1 as well.  The coefficient on the centered covariate &lt;script type=&quot;math/tex&quot;&gt;X_i - \bar{X_i}&lt;/script&gt; comes from the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto &lt;script type=&quot;math/tex&quot;&gt;X_i - \bar{X_i}&lt;/script&gt;.  Equivalently, it is the slope on &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; in the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim 1 + X_i&lt;/script&gt; (think about why this is geometrically).  To summarize, the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim 1 + X_1 + \ldots + X_n&lt;/script&gt; where the covariates &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; are pairwise uncorrelated can be computed by running &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; simple linear regressions &lt;script type=&quot;math/tex&quot;&gt;Y \sim 1 + X_i&lt;/script&gt;.  This slope is &lt;script type=&quot;math/tex&quot;&gt;\frac{Y \cdot (X_i - \bar{X_i})}{(X_i - \bar{X_i}) \cdot(X_i - \bar{X_i})} =\frac{(Y - \bar{Y}) \cdot (X_i - \bar{X_i})}{(X_i - \bar{X_i}) \cdot(X_i - \bar{X_i})} = \frac{\text{Cov}(Y,X_i)}{\text{Var}(X_i)}&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;added-covariate-x_n-is-not-orthogonal-to-previous-covariates&quot;&gt;Added covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; is not orthogonal to previous covariates &lt;/h3&gt;

&lt;p&gt;Now consider the case where &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; is not orthogonal to the previous covariates.  The delta covariate space is spanned by &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; minus the projection of &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; onto the covariate space of the previous covariates.  In other words, the delta covariate space is spanned by the residuals &lt;script type=&quot;math/tex&quot;&gt;\tilde X_n&lt;/script&gt; in the regression &lt;script type=&quot;math/tex&quot;&gt;X_n \sim X_0 + \ldots X_{n-1}&lt;/script&gt; (write &lt;script type=&quot;math/tex&quot;&gt;\tilde{X_n} = X_n - a_0 X_0 - \ldots a_{n-1} X_{n-1}&lt;/script&gt;).  The projection onto the delta covariate space &lt;script type=&quot;math/tex&quot;&gt;\hat Y_{\text{delta}} = b_n \tilde X_n&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; is the coefficient in the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim \tilde X_n&lt;/script&gt;.  Thus&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \hat{Y}_{\text{big}} &amp;= \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}} \\ &amp;= (s_0 X_0 + \ldots + s_{n-1} X_{n-1}) + b_n \tilde X_n\\ &amp;= (s_0 -  b_n a_0) X_0 + \ldots + (s_{n-1} - b_n a_{n-1}) X_{n-1} + b_n X_n \\ &amp;:= b_0 X_0 + \ldots b_n X_n \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;This explains both 2) and 3) above.  The coefficient &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; on a covariate in a regression model can be obtained by regressing &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; on the residuals &lt;script type=&quot;math/tex&quot;&gt;\tilde X_n&lt;/script&gt; from the regression of &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; on the other covariates.  This is often summarized by saying &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; is the effect of &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; on &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; after controlling for the other covariates.  Rather than regress &lt;script type=&quot;math/tex&quot;&gt;Y \sim \tilde{X_n}&lt;/script&gt;, we could instead regress &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \tilde{X_n}&lt;/script&gt; and grab the coefficient on &lt;script type=&quot;math/tex&quot;&gt;\tilde{X_n}&lt;/script&gt;.  This works because &lt;script type=&quot;math/tex&quot;&gt;\tilde{X_n}&lt;/script&gt; is orthogonal to &lt;script type=&quot;math/tex&quot;&gt;X_0&lt;/script&gt;.  In models that include an intercept &lt;script type=&quot;math/tex&quot;&gt;X_0 = 1&lt;/script&gt;, this is what is often done (but is unnecessary).&lt;/p&gt;

&lt;p&gt;Often the residuals &lt;script type=&quot;math/tex&quot;&gt;\tilde Y&lt;/script&gt; (from the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots + X_{n-1}&lt;/script&gt;) are regressed on the residuals &lt;script type=&quot;math/tex&quot;&gt;\tilde X_{n}&lt;/script&gt; to find &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; (see my earlier post on &lt;a href=&quot;/interpreting-regression-coefficients.html&quot;&gt;Interpreting regression coefficients&lt;/a&gt;), rather than just regressing &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; on &lt;script type=&quot;math/tex&quot;&gt;\tilde X_{n}&lt;/script&gt;.  These two regressions estimate the same slope.  (Geometrically this is easy to see: in one case, we are projecting &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the delta covariate space spanned by &lt;script type=&quot;math/tex&quot;&gt;\tilde X_{n}&lt;/script&gt; and in the other, we are projecting &lt;script type=&quot;math/tex&quot;&gt;\tilde Y&lt;/script&gt;.  Both projections are the same because &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\tilde Y&lt;/script&gt; differ by a vector in the small covariate space, which is orthogonal to the delta covariate space we’re projecting onto.)&lt;/p&gt;

&lt;p&gt;We also see how including a new covariate updates the coefficients in the model: &lt;script type=&quot;math/tex&quot;&gt;b_{n-1} = s_{n-1} - b_n a_{n-1}&lt;/script&gt;.  The estimated effect &lt;script type=&quot;math/tex&quot;&gt;s_{n-1}&lt;/script&gt; in the small model does not control for the &lt;strong&gt;omitted variable&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; and must be reduced by &lt;script type=&quot;math/tex&quot;&gt;b_n a_{n-1}&lt;/script&gt; in the big model.  The &lt;strong&gt;omitted variable bias&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;b_n a_{n-1}&lt;/script&gt; is the effect of the included variable &lt;script type=&quot;math/tex&quot;&gt;X_{n-1}&lt;/script&gt; on the response &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; acting through the omitted variable &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; (effect of included on omitted (&lt;script type=&quot;math/tex&quot;&gt;a_{n-1}&lt;/script&gt;) times the effect of omitted on response (&lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt;)).&lt;/p&gt;

&lt;h2 id=&quot;anova&quot;&gt;ANOVA&lt;/h2&gt;
&lt;p&gt;ANOVA was first developed as a way to partition variance in experimental design and later extended to a method to compare linear models (classical ANOVA in experiment design is a special case of the “model comparison” ANOVA where treatments are encoded with dummy factors in a regression model).  Suppose we have two nested models: a small model (with degrees of freedom &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{small}}&lt;/script&gt;) contained in a larger one (with degrees of freedom &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{big}}&lt;/script&gt;).  (The &lt;strong&gt;degrees of freedom&lt;/strong&gt; in a linear model is the dimension of its covariate space or equivalently the number of independent covariates.  If the model includes an intercept (which is not considered a covariate in statistics), the degrees of freedom is the number of independent covariates plus 1 (because the intercept is a covariate from a geometric perspective)).  The larger model will have smaller residuals, but the question is if they are so much smaller that we reject the small model.&lt;/p&gt;

&lt;h2 id=&quot;model-comparison&quot;&gt;Model comparison&lt;/h2&gt;
&lt;p&gt;The “regression sum of squares” is the difference in sum squared residuals between the small and large models:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{SS}_{\text{regression}} &amp;= \text{SS}_{\text{small}} - \text{SS}_{\text{big}} \\&amp;=  \| Y - \hat{Y}_{\text{small}} \|^2 -  \| Y - \hat Y_{\text{big}} \|^2.  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The F-statistic (named after statistician R. A. Fisher) compares regression sum of squares (additional variation explained by the larger model) to the residuals in the larger model (unexplained variation by the larger model):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} F &amp;= \frac{\text{SS}_{\text{regression}} / (\text{df}_{\text{big}} - \text{df}_{\text{small}}) }{\text{SS}_{\text{big}} / (n-\text{df}_{\text{big}}) } \\ &amp;=  \frac{(\text{SS}_{\text{small}} - \text{SS}_{\text{big}}) /(\text{df}_{\text{big}} - \text{df}_{\text{small}}) }{\text{SS}_{\text{big}} / (n-\text{df}_{\text{big}}) }. \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The regression sum of squares is the square length of the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the delta covariate space.  Recall the geometric equation &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{big}} = \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}}&lt;/script&gt;.  In terms of residuals: the residuals in the small model can be decomposed into the residuals in the big model plus the fitted values in the delta model.  Thus &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{small}} = \| Y - \hat{Y}_{\text{small}} \|^2 = \| (Y - \hat{Y}_{\text{big}}) + \hat{Y}_{\text{delta}} \|^2&lt;/script&gt;.  The residuals in the big model are orthogonal to &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{delta}}&lt;/script&gt;, which is contained in the big model.  By the Pythagorean Theorem, we have &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{small}} = \text{SS}_{\text{big}} + \| \hat{Y}_{\text{delta}} \|^2&lt;/script&gt; and so &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{regression}} = \| \hat{Y}_{\text{delta}} \|^2&lt;/script&gt;.  In words, the sum of squares of regression is the square length of the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the delta covariate space between the small and big models.  The &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; statistic is therefore also equal to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F = \frac{\| \hat{Y}_{\text{delta}} \|^2 / (\text{df}_{\text{big}} - \text{df}_{\text{small}}) }{\text{SS}_{\text{big}} /(n-\text{df}_{\text{big}}) }.&lt;/script&gt;

&lt;p&gt;Intuitively, if the F-statistic is near 1, then the big model is not much of an improvement over the small model because the difference in errors between the two models is on the order of the error in the data.  To analyze the F-statistic, we need to assume a statistical model that generates the data.  In the regression framework, we assume &lt;script type=&quot;math/tex&quot;&gt;Y \sim N(\mu, \sigma^2 I)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; lies in some linear subspace.  The small model is correct if &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; belongs to the covariate space of the small model.  The approach is as follows: under the assumption that the small model is correct (null model), we compute the distribution of the F-statistic (spoiler: it follows an F-distribution).  This is called the null distribution of the F-statistic.  We then compare the observed F-statistic to the null distribution and reject the small model if the observed F-statistic is “extreme.”&lt;/p&gt;

&lt;p&gt;Projections (and indeed any linear transformation) of normal random variables are normal. The regression sum of squares is the square length of the normal random variable &lt;script type=&quot;math/tex&quot;&gt;\hat Y_{\text{delta}} = \text{proj}_{L_{\text{delta}}}(Y)&lt;/script&gt;.  Let &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; be the projection matrix so that &lt;script type=&quot;math/tex&quot;&gt;\hat Y_{\text{delta}} = P Y&lt;/script&gt;.  If the small model is correct, &lt;script type=&quot;math/tex&quot;&gt;Y \sim N(\mu, \sigma^2 I)&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;\mu \in L_{\text{small}}&lt;/script&gt;.  Then &lt;script type=&quot;math/tex&quot;&gt;P \hat Y_{\text{delta}} \sim N(P \mu, \sigma^2 P P^T) = N(P \mu, \sigma^2 P)&lt;/script&gt; (projection matrices satisfy &lt;script type=&quot;math/tex&quot;&gt;P = P^T&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;P^2 = P&lt;/script&gt;).  By the spectral theorem from linear algebra, we can write &lt;script type=&quot;math/tex&quot;&gt;P = Q \Lambda Q^T&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; orthogonal and &lt;script type=&quot;math/tex&quot;&gt;\Lambda = \text{Diag}(1,1,\ldots, 1, 0, \ldots, 0)&lt;/script&gt;, a diagonal matrix with &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{big}} - \text{df}_{\text{small}}&lt;/script&gt; 1s followed by 0s on the main diagonal.  Geometrically, we are expressing the projection in three steps: first rotate so the space onto which we are projecting is the standard subspace &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{\text{df}_{\text{big}} - \text{df}_{\text{small}}}&lt;/script&gt;, do the simple projection onto &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{\text{df}_{\text{big}} - \text{df}_{\text{small}}}&lt;/script&gt; by taking the first &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{big}} - \text{df}_{\text{small}}&lt;/script&gt; coordinates and setting the rest to 0, and then rotate back.&lt;/p&gt;

&lt;p&gt;The rotated vector &lt;script type=&quot;math/tex&quot;&gt;Q^T \hat{Y}_{\text{delta}}&lt;/script&gt; is distributed &lt;script type=&quot;math/tex&quot;&gt;N(Q^T P \mu, \sigma^2 \Lambda)&lt;/script&gt;.  Under the assumption that the small model holds and &lt;script type=&quot;math/tex&quot;&gt;\mu \in L_{\text{small}}&lt;/script&gt;, the projection of &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; onto &lt;script type=&quot;math/tex&quot;&gt;L_{\text{delta}}&lt;/script&gt; is 0 and so &lt;script type=&quot;math/tex&quot;&gt;Q^T \hat{Y}_{\text{delta}} / \sigma \sim N(0, \Lambda)&lt;/script&gt;.  The square length &lt;script type=&quot;math/tex&quot;&gt;\| \hat{Y}_{\text{delta}} \|^2 / \sigma^2&lt;/script&gt; is distributed &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{\text{df}_{\text{big}} - \text{df}_{\text{small}}}&lt;/script&gt; (a &lt;script type=&quot;math/tex&quot;&gt;\chi^2_d&lt;/script&gt; random variable with &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; degrees of freedom is the sum of squares of &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; independent standard normal random variables).&lt;/p&gt;

&lt;p&gt;The denominator in the F-statistic is &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{big}}&lt;/script&gt;, the square length of the residuals in the big model.  The residuals in the big model is the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the orthogonal complement of &lt;script type=&quot;math/tex&quot;&gt;L_{\text{big}}&lt;/script&gt; (which I called the residual space before).  Both the small model covariate space and delta covariate space are contained in the big model covariate space.  The residual space is therefore orthogonal to all these spaces.  The variance normalized residuals &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{big}} / \sigma^2&lt;/script&gt; for the big model follow a &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{n - \text{df}_{\text{big}}}&lt;/script&gt; distribution.&lt;/p&gt;

&lt;p&gt;The F-statistic is a ratio of a &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{ \text{df}_{\text{big}} -\text{df}_{\text{small}} } / (\text{df}_{\text{big}} -\text{df}_{\text{small}})&lt;/script&gt; random variable to a &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{n - \text{df}_{\text{big}}} / (n - \text{df}_{\text{big}})&lt;/script&gt; random variable.  This is the definition of an &lt;script type=&quot;math/tex&quot;&gt;F_{\text{df}_{\text{big}} - \text{df}_{\text{small}},\ n - \text{df}_{\text{big}}}&lt;/script&gt; distribution.  (A careful reader will notice that to be F distributed, the numerator and denominator &lt;script type=&quot;math/tex&quot;&gt;\chi^2&lt;/script&gt; distributions must be independent.  This is the case because the two come from independent normal random variables: the numerator from the projection onto the delta covariate space and the denominator from the projection onto the residual space.  These two spaces are orthogonal, and orthogonal zero-mean vectors are uncorrelated.  In the case of normal random variables, uncorrelated means independent.)&lt;/p&gt;

&lt;p&gt;ANOVA is often organized in an ANOVA table.  In practice, you consider a sequence of nested linear models &lt;script type=&quot;math/tex&quot;&gt;M_0 \subset M_1 \subset M_2 \subset M_3 \subset \ldots \subset M_k&lt;/script&gt;.  The inner-most model &lt;script type=&quot;math/tex&quot;&gt;M_0&lt;/script&gt; is always the intercept model, in which &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is estimated with its mean &lt;script type=&quot;math/tex&quot;&gt;\bar{Y}&lt;/script&gt;.  The table has one row for each model &lt;script type=&quot;math/tex&quot;&gt;M_i&lt;/script&gt; (excluding &lt;script type=&quot;math/tex&quot;&gt;M_0&lt;/script&gt;) and a final row for the residuals.  The row for &lt;script type=&quot;math/tex&quot;&gt;M_i&lt;/script&gt; contains information about the numerator of the F-statistic where the big model is &lt;script type=&quot;math/tex&quot;&gt;M_i&lt;/script&gt; and the small model is the previous model &lt;script type=&quot;math/tex&quot;&gt;M_{i-1}&lt;/script&gt;.  It contains the regression sum of squares &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{regression}} = \text{SS}_{M_{i}} - \text{SS}_{M_{i-1}}&lt;/script&gt;, the degrees of freedom &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{regression}} = \text{df}_{M_i} - \text{df}_{M_{i-1}}&lt;/script&gt;, the mean square error &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{regression}} / \text{df}_{\text{regression}}&lt;/script&gt;, the F-statistic, and a P-value.  Unlike we discussed in the previous paragraphs, the F-statistic in the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th row of an ANOVA table does not divide the mean square error for regression by the mean square error for the residuals in the big model &lt;script type=&quot;math/tex&quot;&gt;M_i&lt;/script&gt;.  The denominator instead uses the residuals from the biggest model in the table &lt;script type=&quot;math/tex&quot;&gt;M_k&lt;/script&gt;, which are stored in the last row of the table.  The last row contains the residual sum of squares &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{M_k}&lt;/script&gt;, the residual degrees of freedom &lt;script type=&quot;math/tex&quot;&gt;n - \text{df}_{M_k}&lt;/script&gt;, and the residual mean square error &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{M_k} / (n - \text{df}_{M_k})&lt;/script&gt;, an estimate of &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;.  Here is an example.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/anova_table.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The ANOVA table above is a sequential ANOVA table, in which the models are nested by successively adding new covariates.  The intercept model &lt;script type=&quot;math/tex&quot;&gt;M_0&lt;/script&gt; (not a row in the table) is &lt;script type=&quot;math/tex&quot;&gt;\text{tip} \sim 1&lt;/script&gt;, the first model &lt;script type=&quot;math/tex&quot;&gt;M_1&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\text{tip} \sim 1 + \text{total_bill}&lt;/script&gt;, the second model is &lt;script type=&quot;math/tex&quot;&gt;M_2&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\text{tip} \sim 1 + \text{total_bill} + \text{sex}&lt;/script&gt;, and so forth.&lt;/p&gt;

&lt;h2 id=&quot;power-analysis&quot;&gt;Power analysis&lt;/h2&gt;
&lt;p&gt;We just discussed the distribution of the F-statistic under the small model.  We can reject the small model if the observed F-statistic is extreme for what we expect the F-statistic to look like under the small model.  If we don’t reject the small model, it doesn’t mean that the small model is correct; it just means that we have insufficient evidence to reject it.  The power of a test is the probability that the test rejects the small model (null model) when the big model is true (the alternative model is true).  The probability that we reject the small model if the big model is true will depend on how far the true mean is from the small model covariate space.  Rejection is harder if the true mean is near, but not in, the small model covariate space.&lt;/p&gt;

&lt;p&gt;To do power analysis, we assume that the mean &lt;script type=&quot;math/tex&quot;&gt;\mu \in L_{\text{big}} \setminus L_{\text{small}}&lt;/script&gt; and work out the distribution of the F-statistic.  The denominator of the F-statistic (square norm of the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the residual space) is still a &lt;script type=&quot;math/tex&quot;&gt;\chi^2&lt;/script&gt; distribution because &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; is orthogonal to the residual space.  The numerator of the F-statistic is the square norm of &lt;script type=&quot;math/tex&quot;&gt;Q^T \hat{Y}_{\text{delta}}&lt;/script&gt;, which is distributed &lt;script type=&quot;math/tex&quot;&gt;N(Q^T P \mu, \sigma^2 \Lambda)&lt;/script&gt;.  Under the big model, &lt;script type=&quot;math/tex&quot;&gt;Q^T P \mu&lt;/script&gt; is no longer 0 and we don’t get a &lt;script type=&quot;math/tex&quot;&gt;\chi^2&lt;/script&gt; distribution.  Instead we get a noncentral &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{\text{df}_{\text{big}} - \text{df}_{\text{small}}}( \| P \mu \|^2)&lt;/script&gt; distribution with &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{big}} - \text{df}_{\text{small}}&lt;/script&gt; degrees of freedom and noncentrality parameter &lt;script type=&quot;math/tex&quot;&gt;\| P \mu \|^2&lt;/script&gt;.  Notice that &lt;script type=&quot;math/tex&quot;&gt;\| P \mu \|^2&lt;/script&gt; is just the square distance of &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; to the small model covariate space.  The F-statistic follows a noncentral F distribution.&lt;/p&gt;</content><author><name>Scott Roy</name></author><category term="ANOVA" /><category term="interpreting regression coefficients" /><category term="linear regression" /><category term="omitted variable bias" /><category term="power analysis" /><summary type="html">In this post, I explore the connection of linear regression to geometry.  In particular, I discuss the geometric meaning of fitted values, residuals, and degrees of freedom.  Using geometry, I derive coefficient interpretations and discuss omitted variable bias.  I finish by connecting ANOVA (both hypothesis testing and power analysis) to the underlying geometry.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/linreg_geometry.png" /></entry><entry><title type="html">Inference based on entropy maximization</title><link href="http://localhost:4000/inference-based-on-entropy-maximization.html" rel="alternate" type="text/html" title="Inference based on entropy maximization" /><published>2018-05-18T00:00:00-07:00</published><updated>2018-05-18T00:00:00-07:00</updated><id>http://localhost:4000/inference-based-on-entropy-maximization</id><content type="html" xml:base="http://localhost:4000/inference-based-on-entropy-maximization.html">&lt;h2 id=&quot;entropy&quot;&gt;Entropy&lt;/h2&gt;
&lt;p&gt;For a discrete random variable, the surprisal (or information content) of an outcome with probability &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;-\log p&lt;/script&gt;.  Rare events have a lot surprisal.  For a discrete random variable with &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; outcomes that occur with probabilities &lt;script type=&quot;math/tex&quot;&gt;p_1, \ldots, p_n&lt;/script&gt;, the entropy &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; is the average surprisal&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(p_1,\ldots,p_n) = \sum_{i=1}^n -p_i \log p_i.&lt;/script&gt;

&lt;p&gt;Roughly speaking, entropy measures average unpredictability of a random variable.  For example, the outcome of a fair coin has higher entropy (and is less predictable) than the outcome of a biased coin.  Remarkably, the formula for entropy is determined (up to a multiplicative constant) by a few simple properties:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Entropy is continuous.&lt;/li&gt;
  &lt;li&gt;Entropy is symmetric, which means the value of &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; does not depend on the order of its arguments.  For example, &lt;script type=&quot;math/tex&quot;&gt;H(p_1,\ldots,p_n) = H(p_n, \ldots, p_1)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Entropy is maximized when all outcomes are equally likely.  For equiprobable events, the entropy increases with the number of outcomes.&lt;/li&gt;
  &lt;li&gt;Entropy is consistent in the following sense.  Suppose the event space &lt;script type=&quot;math/tex&quot;&gt;\Omega&lt;/script&gt; is partitioned into sets &lt;script type=&quot;math/tex&quot;&gt;\Omega_1, \ldots, \Omega_k&lt;/script&gt; that occur with probabilities &lt;script type=&quot;math/tex&quot;&gt;\omega_j = \sum_{i \in \Omega_j} p_i&lt;/script&gt;.  Then total entropy is the entropy between the sets plus a weighted average of the entropies within each set:
&lt;script type=&quot;math/tex&quot;&gt;H(p_i : i \in \Omega) = H(\omega_1,\ldots, \omega_k) + \sum_{j=1}^k \omega_j H(p_i : i \in \Omega_j)&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As an aside, variance behaves in the same way&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{Var}(X) = \textbf{Var}(\textbf{E}(X\vert Y)) + \textbf{E}(\textbf{Var}(X\vert Y)),&lt;/script&gt;

&lt;p&gt;a relationship more apparent in the ANOVA setting (where &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; are measurements and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; are group labels): the total variation is the variation between groups plus the the average variation within each group.&lt;/p&gt;

&lt;h2 id=&quot;inference-with-insufficient-data&quot;&gt;Inference with insufficient data&lt;/h2&gt;
&lt;p&gt;Whether a good idea or not, we often want to make inferences with insufficient data.  Doing so requires some kind of external assumption not present in the data.  For example, L1-regularized linear regression solves under-determined linear systems by assuming that the solution is sparse.  Another example is the principle of insufficient reason, which says that in the absence of additional information, we should assume all outcomes of a discrete random variable are equally likely.  In other words, we should assume the distribution with maximum entropy.&lt;/p&gt;

&lt;p&gt;Maximum entropy inference chooses the distribution with maximum entropy subject to what is known.  As an example, suppose that the averages of the functions &lt;script type=&quot;math/tex&quot;&gt;f_k&lt;/script&gt; are known:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n p_i f_k(x_i) = F_k.&lt;/script&gt;

&lt;p&gt;In this case, maximum entropy estimation selects the probability distribution that satisfies&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{max.} &amp;\quad -\sum_{i=1}^n p_i \log p_i \\ \text{s.t.} &amp;\quad \sum_{i=1}^n f_k(x_i) p_i = F_k,\ 1 \leq k \leq K \\ &amp;\quad \sum_{i=1}^n p_i = 1.  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;This convex problem has solution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x) = \frac{1}{Z} e^{\sum_{k=1}^K w_k f_k(x)},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_k&lt;/script&gt; are chosen so that the constraints are satisfied. (We use the notation &lt;script type=&quot;math/tex&quot;&gt;p_i = p(x_i)&lt;/script&gt;.)  Notice that in this case, maximum entropy inference gives the same estimates of &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; that fitting an exponential family using maximum likelihood estimation gives.&lt;/p&gt;

&lt;p&gt;Although maximum entropy estimation lets us answer a question such as “Given the mean of &lt;script type=&quot;math/tex&quot;&gt;f(X)&lt;/script&gt;, what is the mean of &lt;script type=&quot;math/tex&quot;&gt;g(X)&lt;/script&gt;?”, we should always consider whether the answer is meaningful.  For example, when &lt;script type=&quot;math/tex&quot;&gt;f(x) = x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;g(x) = x^2&lt;/script&gt;, we are asking for the variance on the basis of just knowing the mean, and any a priori assumption that makes such a task feasible should be scrutinized.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Information Theory and Statistical Mechanics&lt;/em&gt; by E. T. Jaynes&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Exercise 22.13 in Information Theory, Inference, and Learning Algorithms&lt;/em&gt; by David J. Mackay&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Scott Roy</name></author><category term="entropy" /><category term="exponential family" /><category term="maximum likelihood estimation" /><summary type="html">Entropy For a discrete random variable, the surprisal (or information content) of an outcome with probability is .  Rare events have a lot surprisal.  For a discrete random variable with outcomes that occur with probabilities , the entropy is the average surprisal</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/entropy.png" /></entry><entry><title type="html">Sampling from distributions</title><link href="http://localhost:4000/sampling-from-distributions.html" rel="alternate" type="text/html" title="Sampling from distributions" /><published>2018-05-12T00:00:00-07:00</published><updated>2018-05-12T00:00:00-07:00</updated><id>http://localhost:4000/sampling-from-distributions</id><content type="html" xml:base="http://localhost:4000/sampling-from-distributions.html">&lt;p&gt;There is almost no difference between knowing a distribution’s density (and thus knowing its mean, variance, mode, or anything else about it) and being able to sample from the distribution.  On the one hand, if we can sample from a distribution, we can estimate the density with a histogram or kernel density estimator.  Conversely, I’ll discuss ways to sample from a density in this post.&lt;/p&gt;

&lt;p&gt;I said “almost no difference” because to sample from a density, you do need some external source of randomness.  Sampling techniques do not create randomness, but rather transform one kind of random samples (usually uniform) into samples from a specified density.  As an aside, a uniform random sample &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; can be generated by flipping a coin infinitely many times.  First flip a coin to decide whether to look for &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; in the first or second half of the interval &lt;script type=&quot;math/tex&quot;&gt;(0,1)&lt;/script&gt;.  If the coin shows heads, we look for &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;[0,0.5)&lt;/script&gt;, otherwise we look in &lt;script type=&quot;math/tex&quot;&gt;[0.5,1)&lt;/script&gt;.  This process is repeated recursively.  For example, if the first toss is heads, we toss again to decide whether to look for &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;[0,0.25)&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;[0.25,0.5)&lt;/script&gt;.  In practice, to sample &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; with 9 digits of accuracy, we only need to toss the coin 30 times.&lt;/p&gt;

&lt;p&gt;The observation that we can estimate anything about a distribution by being able to draw samples from it is important for Bayesian statistics in practice.  Bayesian inference is a matter of being able to draw from the posterior.  I’ll now touch on various methods for generating random samples from a given density.&lt;/p&gt;

&lt;h2 id=&quot;use-related-distributions&quot;&gt;Use related distributions&lt;/h2&gt;
&lt;p&gt;There are many relationships between common random variables.  For example,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; is standard normal, then &lt;script type=&quot;math/tex&quot;&gt;\sigma Z + \mu&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;N(\mu, \sigma^2)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; is standard normal and &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; is independently &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{p}&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;T = \frac{Z}{\sqrt{V / p}}&lt;/script&gt; has Student’s t-distribution with &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; degrees of freedom.&lt;/li&gt;
  &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; is uniform, then &lt;script type=&quot;math/tex&quot;&gt;-\frac{1}{\beta} \log(U)&lt;/script&gt; is exponential with rate &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;X \sim \text{Gamma}(\alpha, \lambda)&lt;/script&gt; and independently &lt;script type=&quot;math/tex&quot;&gt;Y \sim \text{Gamma}(\beta, \lambda)&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;\frac{X}{X+Y} \sim \text{Beta}(\alpha, \beta)&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These relationships can be exploited to generate random samples.  For example, to draw &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; from an exponential distribution with rate parameter 1, first draw &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; uniformly over &lt;script type=&quot;math/tex&quot;&gt;(0,1)&lt;/script&gt; and set &lt;script type=&quot;math/tex&quot;&gt;x = -\log(u)&lt;/script&gt;.  Along the same line, a &lt;script type=&quot;math/tex&quot;&gt;\text{Beta}(\alpha, \beta)&lt;/script&gt; random draw can be formed as the ratio of two exponential draws (exponential is a special case of gamma):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\frac{1}{\alpha}\log(u_1)}{\frac{1}{\alpha}\log(u_1) + \frac{1}{\beta}\log(u_2)}&lt;/script&gt;

&lt;h2 id=&quot;inverse-cdf-method&quot;&gt;Inverse CDF method&lt;/h2&gt;
&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; is an invertible CDF and &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; is uniform, then &lt;script type=&quot;math/tex&quot;&gt;X = F^{-1}(U)&lt;/script&gt; is distributed with CDF &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \textbf{P}(X \leq t) &amp;= \textbf{P}(F^{-1}(U) \leq t) \\ &amp;=\textbf{P}(U \leq F(t)) \\ &amp;= F(t) \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Summary: if we know the inverse CDF of a distribution, we can sample from it.  As an aside, the inverse CDF of an exponential distribution is &lt;script type=&quot;math/tex&quot;&gt;F^{-1}(t) = -\frac{1}{\beta} \log(t)&lt;/script&gt;, which we used in the previous section.&lt;/p&gt;

&lt;p&gt;We can approximate the inverse CDF method with a Riemann sum.  Below I approximate a density with rectangles.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/sampling.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The CDF is represented by stacking these rectangles end to end to form a strip.  The inverse is represented in the color-coding.  In the figure, a dot is drawn uniformly on the strip, and is mapped back (using colors) to get a sample from the density.&lt;/p&gt;

&lt;h2 id=&quot;rejection-sampling&quot;&gt;Rejection sampling&lt;/h2&gt;
&lt;p&gt;Rejection sampling is a technique to sample from a subset, assuming we know how to sample from the larger set.  I’ll explain the technique with an example.  Suppose I want to sample a random point from a lightning bolt subset of a square.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/accept_reject.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In rejection sampling, I first draw a point uniformly at random from the square, and I accept the sample if it lands in the lightning bolt, and reject the sample otherwise.  The accepted samples are distributed uniformly over the lightning bolt.The probability that a sample is accepted is the area of the lightning bolt over the area of the square.  The acceptance probability measures the efficiency of the method: if I want &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; random samples from the lightning bolt, on average I have to draw &lt;script type=&quot;math/tex&quot;&gt;\frac{n}{\textbf{P}(\text{acceptance})}&lt;/script&gt; samples from the square.&lt;/p&gt;

&lt;p&gt;Rejection sampling is often introduced as a technique for drawing from a density.  Drawing from a density is equivalent to drawing uniformly from the region below the density.  To be more precise, drawing a random variable from a density &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is equivalent to drawing uniformly from the region below the density in the following sense:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;(x,y)&lt;/script&gt; is drawn uniformly over the region below the density, then &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is a random draw from &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is a random draw from &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is a uniform draw from the interval &lt;script type=&quot;math/tex&quot;&gt;(0, f(x))&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;(x, y)&lt;/script&gt; is uniformly distributed over the region below the density.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To sample from &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; with rejection sampling, we first find a proposal density &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; and constant &lt;script type=&quot;math/tex&quot;&gt;c \geq 1&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;cg \geq f&lt;/script&gt;.  We then&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sample from &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; to construct a uniform sample under &lt;script type=&quot;math/tex&quot;&gt;c g&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Apply rejection sampling to get a uniform sample under &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;The x-coordinate of the uniform sample under &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is a sample from &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/accept_reject2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The acceptance rate is &lt;script type=&quot;math/tex&quot;&gt;1/c&lt;/script&gt;, the area under &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; over the area under &lt;script type=&quot;math/tex&quot;&gt;cg&lt;/script&gt;.  The more &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; is like &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;, the more efficient the rejection sampling algorithm.&lt;/p&gt;

&lt;p&gt;Constructing a proposal density &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; that is 1) easy to sample from and 2) closely resembles &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; can be challenging.  Let’s walk through an example with the standard normal density &lt;script type=&quot;math/tex&quot;&gt;f(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}&lt;/script&gt;.  Since &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is log-concave, we can construct a piecewise exponential curve &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; that upper bounds (and well approximates) &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.  This piecewise exponential curve can be sampled from quickly using the inverse CDF method.&lt;/p&gt;

&lt;h2 id=&quot;adaptive-rejection-sampling&quot;&gt;Adaptive rejection sampling&lt;/h2&gt;
&lt;p&gt;Adaptive rejection sampling is a variant of rejection sampling that learns the proposal density &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; as it runs.  The method applies to log-concave densities and constructs a piecewise exponential &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; by constructing a piecewise linear function that approximates (and upper bounds) &lt;script type=&quot;math/tex&quot;&gt;h(x) = \log f(x)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/adaptive_sampling.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;issues-in-high-dimensions&quot;&gt;Issues in high dimensions&lt;/h2&gt;
&lt;p&gt;Rejection sampling is generally inefficient in high dimensions.  To explain, suppose I sample from standard normal by using a normal with variance &lt;script type=&quot;math/tex&quot;&gt;\sigma^2 \geq 1&lt;/script&gt; as the proposal density.  (This example is just to illustrate a point.  We can get a standard normal sample from any other normal sample by “standardizing,” and do not need to use rejection sampling.)  The standard normal has density &lt;script type=&quot;math/tex&quot;&gt;f(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}&lt;/script&gt; and the proposal density is &lt;script type=&quot;math/tex&quot;&gt;g(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{-x^2/(2\sigma^2)}&lt;/script&gt;.  Suppose &lt;script type=&quot;math/tex&quot;&gt;\sigma = 1.001&lt;/script&gt;.  In this case, we can set &lt;script type=&quot;math/tex&quot;&gt;c = \sigma&lt;/script&gt; so that &lt;script type=&quot;math/tex&quot;&gt;cg \geq f&lt;/script&gt; and the acceptance probability is &lt;script type=&quot;math/tex&quot;&gt;1 / c \approx 99.9\%&lt;/script&gt;.  Now let &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; be the density of a 5000-dimensional normal random vector with covariance matrix &lt;script type=&quot;math/tex&quot;&gt;I&lt;/script&gt; and suppose &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; has covariance matrix &lt;script type=&quot;math/tex&quot;&gt;\sigma^2I&lt;/script&gt;.  In this case &lt;script type=&quot;math/tex&quot;&gt;c = \sigma^{5000}&lt;/script&gt; and the acceptance rate &lt;script type=&quot;math/tex&quot;&gt;1 / c&lt;/script&gt; is less than a percent.&lt;/p&gt;

&lt;p&gt;In high dimensional problems that arise in Bayesian sampling, methods based off Markov chains such as MCMC or Gibbs sampling are used.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Adaptive Rejection Sampling for Gibbs Sampling&lt;/em&gt; by W. R. Gilks and P. Wild&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Scott Roy</name></author><category term="adaptive rejection sampling" /><category term="rejection method" /><summary type="html">There is almost no difference between knowing a distribution’s density (and thus knowing its mean, variance, mode, or anything else about it) and being able to sample from the distribution.  On the one hand, if we can sample from a distribution, we can estimate the density with a histogram or kernel density estimator.  Conversely, I’ll discuss ways to sample from a density in this post.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/sampling.png" /></entry><entry><title type="html">ROC space and AUC</title><link href="http://localhost:4000/ROC-space-and-AUC.html" rel="alternate" type="text/html" title="ROC space and AUC" /><published>2018-04-29T00:00:00-07:00</published><updated>2018-04-29T00:00:00-07:00</updated><id>http://localhost:4000/ROC-space-and-AUC</id><content type="html" xml:base="http://localhost:4000/ROC-space-and-AUC.html">&lt;p&gt;Before discussing ROC curves and AUC, let’s fix some terminology around the confusion matrix:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Condition positive (negative):&lt;/strong&gt; real positive (negative) case in the data&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;True positive (negative):&lt;/strong&gt; condition positive (negative) that is classified as positive (negative)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;False positive (negative):&lt;/strong&gt; condition negative (positive) that is classified as positive (negative)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;True positive rate (TPR):&lt;/strong&gt; empirically defined as (# true positives) / (# condition positives).  More generally, it is the probability that a condition positive is labelled as positive.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;False positive rate (FPR):&lt;/strong&gt; empirically defined as (# false positives) / (# condition negatives).  More generally, it is the probability that a condition negative is classified as positive.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/confusion.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;True positive rate and false positive rate do not depend on the class ratio, that is, the ratio of condition positives to condition negatives in the data. Contrast this with precision, the number of true positives over the number of predicted positives, which does depend on the class ratio.&lt;/p&gt;

&lt;h2 id=&quot;roc-space&quot;&gt;ROC Space&lt;/h2&gt;

&lt;p&gt;The concept of ROC (receiver operator characteristic) space was introduced by engineers during WWII.  Radar operators had to decide if a blip on a radar screen was an enemy target or not. The performance of different operators can be compared by plotting an operator’s true positive rate along the y-axis and false positive rate along the x-axis.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/random_guessers.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In “ROC space,” the better operators are in the upper, left corner (high true positive rate, low false positive rate). Any decent operator lies in the upper, left region above the diagonal. In fact, points on the diagonal correspond to the performance of “random guessers.” Suppose Jim flips a coin and classifies a blip as an enemy target if the coin shows heads. In this case, Jim will classify half of condition positives as targets (50% true positive rate) and half of condition negatives as targets (50% false positive rate). If the coin is biased and has a 70% chance of showing heads, Jim’s TPR and FPR will both be 70%. The entire diagonal line corresponds to the performance of random guessers who use biased coins with varying probabilities of showing heads.&lt;/p&gt;

&lt;h2 id=&quot;roc-curves&quot;&gt;ROC Curves&lt;/h2&gt;
&lt;p&gt;Like radar operators, we can compare the performance of classifiers by plotting TPR and FPR in ROC space. Before discussing ROC curves, I want to distinguish between a classifier and a scorer. A classifier labels an input as positive or negative. Its output is binary. Many classifiers in machine learning are built from scorers. A scorer assigns each input a score that measures a “belief” that the input is a positive example. For example, a logistic regression scorer assigns each input a probability score of being a positive example. (As an aside, the probability scores computed by logistic regression (or any machine learning model) need not be well-calibrated, true probabilities. For this reason, care should be taken when comparing scores from different models.)  We get a logistic regression classifier, for example, by labelling an input as positive if its score is greater than 0.5.  A scorer generates many classifiers indexed by a threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, where the classifier at threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; labels an input as positive or negative depending on if its score is above or below &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We can plot the performance of all the classifiers generated by a scorer in ROC space.  For high values of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, few examples are classified as positive, and so both the true positive rate and false positive rate are low. Similarly, the true positive rate and false positive rate are are high for low values of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.  So the ROC curve for a scorer starts in the upper right corner, ends in the lower left corner, and will (hopefully) bulge out toward the upper left corner.&lt;/p&gt;

&lt;h2 id=&quot;auc&quot;&gt;AUC&lt;/h2&gt;
&lt;p&gt;A good scorer will have an ROC curve that bulges out toward the upper left corner.  The AUC is the area under the ROC curve, and is a heuristic for measuring how much the curve bulges toward the upper left corner.  A perfect scorer has AUC 1.&lt;/p&gt;

&lt;p&gt;AUC has an alternative interpretation: it is the probability that a condition positive has a higher score than a condition negative.  Let’s see why. Let &lt;script type=&quot;math/tex&quot;&gt;\beta(t)&lt;/script&gt; be the TPR at threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\alpha(t)&lt;/script&gt; be the FPR at threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. (Notice that the use of &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; here is consistent with their use to denote Type I error and power in statistics.)
Let &lt;script type=&quot;math/tex&quot;&gt;f_{+}(x)&lt;/script&gt; be the score density of the condition positives.  Then we can write &lt;script type=&quot;math/tex&quot;&gt;\beta(t) = \int_{t}^{\infty} f_{+}(x)\ dx&lt;/script&gt;. Similarly, &lt;script type=&quot;math/tex&quot;&gt;\alpha(t) = \int_t^{\infty} f_{-}(x) \ dx&lt;/script&gt;.  The AUC is the integral over infinitesimal rectangles under of the ROC curve. The rectangle at &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; has height &lt;script type=&quot;math/tex&quot;&gt;\beta(t)&lt;/script&gt; and width &lt;script type=&quot;math/tex&quot;&gt;d\alpha = f_{-}(t) dt&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/auc.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Putting things together, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\text{AUC} &amp;= \int \beta(t) f_{-}(t) dt \\ &amp;= \int \textbf{P}(\text{condition positive has score greater than } t)\ \textbf{P}(\text{condition negative has score } t) dt \\ &amp;= \textbf{P}(\text{condition positive has higher score than condition negative}). \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;ROC curves and AUC (being based on TPR and FPR) do not depend on the class ratio in the data.&lt;/p&gt;

&lt;h2 id=&quot;picking-the-best-classifier-in-roc-space&quot;&gt;Picking the “best” classifier in ROC space&lt;/h2&gt;
&lt;p&gt;A classifier above &lt;strong&gt;and&lt;/strong&gt; to the left of another in ROC space is objectively better.  But not all points in ROC space are comparable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/better_worse_roc_space.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For example, a classifier can have a better TPR than another, but a worse FPR.  Choosing between incomparable classifiers depends on the specific problem.  In particular, it depends on the cost of a false positive, the cost of a false negative, and the ratio of condition positives to condition negatives in the data.  As an example, consider a classifier that predicts if a patient has HIV.  In this case, the cost of a false negative (failing to detect an HIV positive patient) is significantly more than the cost of a false positive (incorrectly diagnosing HIV).&lt;/p&gt;

&lt;p&gt;Let’s model expected total cost.  Each mistake has a cost.  Let &lt;script type=&quot;math/tex&quot;&gt;M_{+}&lt;/script&gt; be the number of false positives and &lt;script type=&quot;math/tex&quot;&gt;c_{+}&lt;/script&gt; be the cost of a false positive (with similar definitions for &lt;script type=&quot;math/tex&quot;&gt;M_{-}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;c_{-}&lt;/script&gt;).  In expectation, &lt;script type=&quot;math/tex&quot;&gt;M_{+}&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;n p_{-} \alpha&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is the number of classified points, &lt;script type=&quot;math/tex&quot;&gt;p_{-}&lt;/script&gt; is the probability of a condition negative, and &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is the true positive rate.  Similarly, the expected value of &lt;script type=&quot;math/tex&quot;&gt;M_{-}&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;n p_{+} (1-\beta)&lt;/script&gt;.  The average total cost is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;c_{+} n p_{-} \alpha + c_{-} n p_{+} (1-\beta),&lt;/script&gt;

&lt;p&gt;and we can find the best classifier by minimizing this over ROC space.  Equivalently, we can maximize the linear functional&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\alpha, \beta) \mapsto -\left( \frac{c_{+}}{c_{-}} \right) \left( \frac{p_{-}}{p_{+}} \right) \alpha + \beta.&lt;/script&gt;

&lt;p&gt;Notice that the selection of the best classifier only depends on the cost ratio &lt;script type=&quot;math/tex&quot;&gt;c_{+} / c_{-}&lt;/script&gt; and the class ratio &lt;script type=&quot;math/tex&quot;&gt;p_{-} / p_{+}&lt;/script&gt;. In the special case where the classes are balanced and a false positive has the same cost as a false negative, the best classifier is the one furthest in the northeast direction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/cost_roc_space.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;interpolating-classifiers-in-roc-space&quot;&gt;Interpolating classifiers in ROC space&lt;/h2&gt;
&lt;p&gt;Given any two classifiers in ROC space, we can interpolate on the line segment between them.  Suppose classifier 1 is at &lt;script type=&quot;math/tex&quot;&gt;(\alpha_1, \beta_1)&lt;/script&gt; and classifier 2 is at &lt;script type=&quot;math/tex&quot;&gt;(\alpha_2, \beta_2)&lt;/script&gt;.  We can form a combined classifier that lies at &lt;script type=&quot;math/tex&quot;&gt;(\theta \alpha_1 + (1-\theta) \alpha_2, \theta \beta_1 + (1-\theta) \beta_2)&lt;/script&gt; as follows.  We flip a coin that shows heads with probability &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and predict the output of classifier 1 if the coin shows heads, and predict the output of classifier 2 if the coin shows tails. In this way, we can get the performance of any point in the convex hull of the classifiers we plot in ROC space.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Measuring classifier performance: a coherent alternative to the area under the ROC curve&lt;/em&gt; by David J. Hand&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;An introduction to ROC analysis&lt;/em&gt; by Tom Fawcett&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Scott Roy</name></author><category term="AUC" /><category term="false positive rate" /><category term="ROC" /><category term="true positive rate" /><category term="type I error" /><category term="type II error" /><summary type="html">Before discussing ROC curves and AUC, let’s fix some terminology around the confusion matrix:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/cost_roc_space.png" /></entry><entry><title type="html">Propagation of error</title><link href="http://localhost:4000/propogation-of-error.html" rel="alternate" type="text/html" title="Propagation of error" /><published>2018-04-28T00:00:00-07:00</published><updated>2018-04-28T00:00:00-07:00</updated><id>http://localhost:4000/propogation-of-error</id><content type="html" xml:base="http://localhost:4000/propogation-of-error.html">&lt;p&gt;Propagation of error describes how uncertainty in estimates propagates forward when we consider functions of those estimates.&lt;/p&gt;

&lt;p&gt;Suppose I have height and weight measurements for a sample of people.  What is the mean and variance of the BMI?  (BMI is 703 times the weight (in pounds) over the square of the height (in inches).)&lt;/p&gt;

&lt;p&gt;The obvious thing is to compute the BMI for each individual in the data set, and then compute the mean of and variance of the BMIs.  But suppose that I don’t have access to the original data, but only have summary statistics about the heights and weights.  What can I say about BMI then?&lt;/p&gt;

&lt;p&gt;Propagation of error is derived by taking a first-order Taylor expansion about the mean.  Suppose &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is a random variable with mean &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and variance &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;, and we want to approximate the mean and variance of &lt;script type=&quot;math/tex&quot;&gt;f(X)&lt;/script&gt;.  We know &lt;script type=&quot;math/tex&quot;&gt;f(X) \approx f(\mu) + f'(\mu)(X- \mu)&lt;/script&gt;, so&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\textbf{E}(f(X)) &amp;\approx \textbf{E}(f(\mu) + f'(\mu)(X- \mu)) = f(\mu) \\
\textbf{Var}(f(X)) &amp;\approx \textbf{Var}(f(\mu) + f'(\mu)(X- \mu)) = f'(\mu)^2 \textbf{Var}(X).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;As a sanity check, notice that &lt;script type=&quot;math/tex&quot;&gt;\textbf{Var}(f(X))&lt;/script&gt; is modulated by &lt;script type=&quot;math/tex&quot;&gt;\vert f'(\mu)\vert&lt;/script&gt;.  This makes sense: if &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is flat near &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;, the “range” of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is compressed (less variation), and if &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is steep, the “range” is expanded (more variation).  How good are these approximations?  This depends on how non-linear &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is and how central &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is about its mean.  But the approximations are good enough for the central limit theorem.&lt;/p&gt;

&lt;p&gt;Consider a central limit theorem type statement like &lt;script type=&quot;math/tex&quot;&gt;\sqrt{n} (X_n - \theta)&lt;/script&gt; converges to &lt;script type=&quot;math/tex&quot;&gt;N(0, \sigma^2)&lt;/script&gt; in distribution (this is the central limit theorem if &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; is the sample mean of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; data points and &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is the true mean.)  If we consider some function of &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt;, we still have a limit theorem:  &lt;script type=&quot;math/tex&quot;&gt;\sqrt{n} (f(X_n) - f(\theta))&lt;/script&gt; converges to &lt;script type=&quot;math/tex&quot;&gt;N(0, f'(\theta)^2 \sigma^2)&lt;/script&gt; (provided the derivative is non-zero).  The proof is just a Taylor expansion about &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\sqrt{n} (f(X_n) - f(\theta)) &amp;= \sqrt{n} (f(\theta) + f'(\theta)(X_n - \theta) + \epsilon - f(\theta)) \\
&amp;= f'(\theta) \sqrt{n} (X_n - \theta) + \sqrt{n} \epsilon
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The first term &lt;script type=&quot;math/tex&quot;&gt;f'(\theta) \sqrt{n} (X_n - \theta)&lt;/script&gt; converges in distribution to &lt;script type=&quot;math/tex&quot;&gt;f'(\theta) N(0, \sigma^2) = N(0, f'(\theta)^2 \sigma^2)&lt;/script&gt;.  The error &lt;script type=&quot;math/tex&quot;&gt;\sqrt{n} \epsilon&lt;/script&gt; converges to 0 in probability.  I think the easiest way to see this is to write &lt;script type=&quot;math/tex&quot;&gt;\sqrt{n} \epsilon&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;\vert \sqrt{n} (X_n - \theta)\vert   \cdot \left(\epsilon / \vert X_n - \theta\vert  \right)&lt;/script&gt;.  The first factor &lt;script type=&quot;math/tex&quot;&gt;\vert \sqrt{n} (X_n - \theta)\vert&lt;/script&gt; converges in distribution to the absolute value of a normal random variable, and the second factor &lt;script type=&quot;math/tex&quot;&gt;\epsilon / \vert X_n - \theta\vert&lt;/script&gt; converges to 0 in probability.&lt;/p&gt;

&lt;p&gt;Using a multivariate Taylor expansion, we have the same statements in higher dimensions: if &lt;script type=&quot;math/tex&quot;&gt;\sqrt{n} (X_n - \theta)&lt;/script&gt; converges in distribution to a multivariate normal &lt;script type=&quot;math/tex&quot;&gt;N(0, \Sigma)&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;\sqrt{n}(f(X_n) - f(\theta))&lt;/script&gt; converges to a univariate normal &lt;script type=&quot;math/tex&quot;&gt;N(0, \nabla f(\theta)^T \Sigma \nabla f(\theta))&lt;/script&gt;.  Let’s work through the height, weight, and BMI example to make this more concrete.&lt;/p&gt;

&lt;p&gt;Suppose we have summary statistics about the weights and heights: &lt;script type=&quot;math/tex&quot;&gt;\mu_w = 164.39&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\sigma_w = 23.58&lt;/script&gt;,  &lt;script type=&quot;math/tex&quot;&gt;\mu_h = 70.45&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\sigma_h = 3.03&lt;/script&gt;, and correlation &lt;script type=&quot;math/tex&quot;&gt;\rho = 0.40&lt;/script&gt;.  Let &lt;script type=&quot;math/tex&quot;&gt;f(w, h) = 703 w / h^2&lt;/script&gt; be the BMI.  The covariance matrix and gradient at the mean are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\Sigma &amp;= \begin{bmatrix} \sigma_w^2 &amp; \rho \sigma_w \sigma_h \\ \rho \sigma_w \sigma_h &amp; \sigma_h^2 \end{bmatrix} =\begin{bmatrix} 556.02 &amp; 28.58 \\ 28.58 &amp; 9.18 \end{bmatrix} \\
\nabla f(\theta) &amp;= 703 \begin{bmatrix} 1/\mu_h^2 \\ -2 \mu_w / \mu_h^3 \end{bmatrix} = \begin{bmatrix} 0.14 \\ -0.66 \end{bmatrix}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;By propagation of error combined with the central limit theorem, the mean BMI of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; people is approximately normally distributed with mean &lt;script type=&quot;math/tex&quot;&gt;f(\theta) = (703)(164.39) / (70.45^2) = 23.28&lt;/script&gt; and variance &lt;script type=&quot;math/tex&quot;&gt;\nabla f(\theta)^T \Sigma \nabla f(\theta)) / n = 9.82 / n&lt;/script&gt;.  The approximation is better the larger &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is.&lt;/p&gt;</content><author><name>Scott Roy</name></author><category term="central limit theorem" /><category term="propogration of error" /><summary type="html">Propagation of error describes how uncertainty in estimates propagates forward when we consider functions of those estimates.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bmi.png" /></entry><entry><title type="html">Maximum likelihood estimation with censored data</title><link href="http://localhost:4000/maximum-likelihood-estimation-with-censored-data.html" rel="alternate" type="text/html" title="Maximum likelihood estimation with censored data" /><published>2018-04-20T00:00:00-07:00</published><updated>2018-04-20T00:00:00-07:00</updated><id>http://localhost:4000/maximum-likelihood-estimation-with-censored-data</id><content type="html" xml:base="http://localhost:4000/maximum-likelihood-estimation-with-censored-data.html">&lt;p&gt;Suppose I’m tasked with analyzing failure times for hard drives in a datacenter.  I track 100 hard drives over a 2 year period, and if a hard drive fails, I record when. If the hard drive has not failed by the 2 year mark, I don’t when when it will fail, just that its failure time is more than 2 years. We say the failure times for the hard drives remaining at the 2 year mark are censored.&lt;/p&gt;

&lt;p&gt;I decide to fit an exponential model to the failure times, and I fit the parameter with maximum likelihood estimation. The exponential distribution with rate parameter &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; has density &lt;script type=&quot;math/tex&quot;&gt;f(x \vert  \beta) = \beta e^{-\beta x}&lt;/script&gt;. To keep things simple, suppose I observe failures for the first 75 hard drives (with failure times &lt;script type=&quot;math/tex&quot;&gt;x_1,\ldots, x_{75}&lt;/script&gt; in years),  and the last 25 hard drives have censored failure times.  The likelihood is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
f(x\vert \beta) &amp;= \left( \prod_{i=1}^{75} f(x_i \vert  \beta) \right) \textbf{P}(X \geq 2 \vert  \beta)^{25} \\
&amp;= \left(\prod_{i=1}^{75} \beta e^{-\beta x_i} \right) \left( e^{-2 \beta} \right)^{25}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The negative log likelihood is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(x\vert \beta) = \left(\sum_{i=1}^{75} x_i + 25 \times 2 \right) \beta - 75 \log (\beta),&lt;/script&gt;

&lt;p&gt;which means the MLE is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\beta} = \frac{75}{\sum_{i=1}^{75} x_i + 25 \times 2}.&lt;/script&gt;

&lt;p&gt;I want to compare this to the estimates I get if I improperly handle the censored data in each of the following ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Discard the censored observations.&lt;/li&gt;
  &lt;li&gt;Record the censored observations as 2 years.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If I discard the censored observations, the negative log likelihood and MLE are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
l(x \vert  \beta) &amp;= \left(\sum_{i=1}^{75} x_i \right) \beta - 75 \log (\beta) \\
\hat{\beta} &amp;= \frac{75}{\sum_{i=1}^{75} x_i}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;In this case, I over-estimate the exponential rate &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; (i.e., under-estimate the mean &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{\beta}&lt;/script&gt;).  I also under-estimate the mean if I replace the censored observations with 2 years:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
l(x \vert  \beta) &amp;= \left(\sum_{i=1}^{75} x_i + 25 \times 2 \right) \beta - 100 \log (\beta) \\
\hat{\beta} &amp;= \frac{100}{\sum_{i=1}^{75} x_i+ 25 \times 2}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can deal with other kinds of censoring in MLE calculations, too. Suppose we don’t know what &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; is, but we just know that it lies between 3 and 4. We would then use &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}( 3 \leq X \leq 4 \vert  \beta)&lt;/script&gt; instead of &lt;script type=&quot;math/tex&quot;&gt;f(x_i \vert  \beta)&lt;/script&gt; in the likelihood product.&lt;/p&gt;</content><author><name>Scott Roy</name></author><category term="censored data" /><category term="maximum likelihood estimation" /><category term="MLE" /><summary type="html">Suppose I’m tasked with analyzing failure times for hard drives in a datacenter.  I track 100 hard drives over a 2 year period, and if a hard drive fails, I record when. If the hard drive has not failed by the 2 year mark, I don’t when when it will fail, just that its failure time is more than 2 years. We say the failure times for the hard drives remaining at the 2 year mark are censored.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/censored_data.png" /></entry><entry><title type="html">Autoencoders</title><link href="http://localhost:4000/autoencoders.html" rel="alternate" type="text/html" title="Autoencoders" /><published>2018-04-13T00:00:00-07:00</published><updated>2018-04-13T00:00:00-07:00</updated><id>http://localhost:4000/autoencoders</id><content type="html" xml:base="http://localhost:4000/autoencoders.html">&lt;p&gt;High dimensional data is usually very structured, and this structure allows us to map the data to a smaller dimensional space without losing much information.  For example, if this post is encoded with ASCII, it lives in a &lt;script type=&quot;math/tex&quot;&gt;(256)^{\text{length post}}&lt;/script&gt; point space.  But since I’m writing in English, the sequence of characters “Life is a tale told by an idiot, full of sound and fury, signifying nothing” is far more likely than the sequence “fjkla;8#knxHJD38lkdf dfjkal.”  Thus my post really lives in a very low-dimensional “English subspace” of the full &lt;script type=&quot;math/tex&quot;&gt;(256)^{\text{length post}}&lt;/script&gt; point space.  The same principle applies to lots of other data.  For example, strictly speaking a 10 megapixel image is described by about 30 million numbers (about 10 million RGB triples), but since people take pictures of predictable things such as cats, dogs, faces, rocks, etc., you can describe the picture using much fewer than 30 million numbers.  Dimensionality reduction is used for image compression, audio compression, and video compression.  In learning algorithms, we have high dimensional feature vectors (e.g., the features could be the RGB values in an image), but the algorithms behave better if we first map the features to a low-dimensional space.&lt;/p&gt;

&lt;p&gt;The problem is to map &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; dimensional data to a &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; dimensional code, and the goal is to learn an encoder &lt;script type=&quot;math/tex&quot;&gt;f : \mathbb{R}^N \to \mathbb{R}^d&lt;/script&gt; and a decoder &lt;script type=&quot;math/tex&quot;&gt;g : \mathbb{R}^d \to \mathbb{R}^N&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;g(f(x)) \approx x&lt;/script&gt;.  PCA (principal component analysis) is historically the most popular method of dimensionality reduction.  In PCA, both the encoder and decoder are linear maps and geometrically it’s finding a &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; dimensional hyperplane that the &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; dimensional data hovers around.  A more modern approach uses autoencoders.&lt;/p&gt;

&lt;p&gt;An autoencoder is a feed-forward neural network that we train so that the &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; dimensional output layer reconstructs the &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;-dimensional input layer.  Internally there is a hidden “code” layer with &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; nodes.  The encoder is the “bottom” part of the network mapping the &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; dimensional input layer to the &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; dimensional code layer, and the decoder is the “top” part of the network mapping the &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; dimensional code layer to the &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; dimensional output layer.  In training the autoencoder, we minimize&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(x, g(f(x))),&lt;/script&gt;

&lt;p&gt;where the loss &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; tries to force the reconstruction &lt;script type=&quot;math/tex&quot;&gt;g(f(x))&lt;/script&gt; to be close the input &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;.  To prevent the autoencoder from learning the identity, we impose a structural constraint in the network by making &lt;script type=&quot;math/tex&quot;&gt;d \ll N&lt;/script&gt;, or we add regularization to the loss function and try to minimize&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(x, g(f(x))) + \Omega(h),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;h = f(x)&lt;/script&gt; is the hidden code layer, and &lt;script type=&quot;math/tex&quot;&gt;\Omega&lt;/script&gt; imposes some structure on &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;, e.g., sparsity.&lt;/p&gt;</content><author><name>Scott Roy</name></author><summary type="html">High dimensional data is usually very structured, and this structure allows us to map the data to a smaller dimensional space without losing much information.  For example, if this post is encoded with ASCII, it lives in a point space.  But since I’m writing in English, the sequence of characters “Life is a tale told by an idiot, full of sound and fury, signifying nothing” is far more likely than the sequence “fjkla;8#knxHJD38lkdf dfjkal.”  Thus my post really lives in a very low-dimensional “English subspace” of the full  point space.  The same principle applies to lots of other data.  For example, strictly speaking a 10 megapixel image is described by about 30 million numbers (about 10 million RGB triples), but since people take pictures of predictable things such as cats, dogs, faces, rocks, etc., you can describe the picture using much fewer than 30 million numbers.  Dimensionality reduction is used for image compression, audio compression, and video compression.  In learning algorithms, we have high dimensional feature vectors (e.g., the features could be the RGB values in an image), but the algorithms behave better if we first map the features to a low-dimensional space.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/autoencoder.png" /></entry><entry><title type="html">Distinguishing proportions: the risk ratio</title><link href="http://localhost:4000/distinguishing-proportions-the-risk-ratio.html" rel="alternate" type="text/html" title="Distinguishing proportions: the risk ratio" /><published>2018-04-13T00:00:00-07:00</published><updated>2018-04-13T00:00:00-07:00</updated><id>http://localhost:4000/distinguishing-proportions-the-risk-ratio</id><content type="html" xml:base="http://localhost:4000/distinguishing-proportions-the-risk-ratio.html">&lt;p&gt;Suppose I conduct an experiment to determine whether to use font A or font B in an online ad.  After running the experiment, I find that there is a 1% chance that a user clicks on the font A ad, and a 0.8% chance that the user clicks on the font B ad.&lt;/p&gt;

&lt;p&gt;We can can compare these click rates on an absolute scale (font A increases the click rate by 0.2%) or compare on a relative scale (the click rate for font A is 1.25 times higher).  The relative comparison (the probability of clicking on a font A ad over the probability of clicking on a font B ad) is called the risk ratio in some fields.  To determine statistical significance, we can see if the difference is significantly different than 0 or if the ratio is significantly different than 1.  I’d highly recommend comparing the difference to 0 (in fact, we’ll handle the ratio by taking the log and turning it into a difference), but the ratio is nice for reporting purposes and so I’ll discuss computing a confidence interval for it.&lt;/p&gt;

&lt;p&gt;Suppose I show &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; impressions of the ad with font A, and let &lt;script type=&quot;math/tex&quot;&gt;X_1, \ldots, X_n&lt;/script&gt; be the outcomes (&lt;script type=&quot;math/tex&quot;&gt;X_1 = 1&lt;/script&gt; means the user clicked on the ad and &lt;script type=&quot;math/tex&quot;&gt;X_1=0&lt;/script&gt; means the user did not click on the ad).  Similarly, let &lt;script type=&quot;math/tex&quot;&gt;Y_1, \ldots, Y_m&lt;/script&gt; be the outcomes of the &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; impressions of the ad with font B I show.&lt;/p&gt;

&lt;p&gt;The estimated probability of clicking on the font A ad is &lt;script type=&quot;math/tex&quot;&gt;\hat{p}_A = \sum X_i / n&lt;/script&gt;.  By the central limit theorem, &lt;script type=&quot;math/tex&quot;&gt;\hat{p}_A&lt;/script&gt; is approximately distributed &lt;script type=&quot;math/tex&quot;&gt;N(p_A,\ p_A(1-p_A) / n)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;p_A&lt;/script&gt; is the true probability.  Similarly, &lt;script type=&quot;math/tex&quot;&gt;\hat{p}_B&lt;/script&gt; is approximately distributed &lt;script type=&quot;math/tex&quot;&gt;N(p_B,\ p_B(1-p_B) / m)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Thus the difference is approximately distributed&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}_A - \hat{p}_B\ \dot\sim\ N(p_A - p_B,\ p_A(1-p_A) / n + p_B(1-p_B) / m).&lt;/script&gt;

&lt;p&gt;Under the null hypothesis, we assume &lt;script type=&quot;math/tex&quot;&gt;p_A = p_B&lt;/script&gt;.  If we let &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; denote this common value, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\hat{p}_A - \hat{p}_B}{\sqrt{p(1-p) \left(\frac{1}{n} + \frac{1}{m}\right)}} \ \dot\sim\ N(0, 1).&lt;/script&gt;

&lt;p&gt;Slutsky’s theorem lets us replace &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; with the pooled estimate &lt;script type=&quot;math/tex&quot;&gt;\hat{p} = (\sum X_i + \sum Y_i) / (n + m)&lt;/script&gt;.  In summary, to test if the difference &lt;script type=&quot;math/tex&quot;&gt;\hat{p}_A - \hat{p}_B&lt;/script&gt; is significantly different than &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;, we compute the &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt;-score&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\hat{p}_A - \hat{p}_B}{\sqrt{\hat{p}(1-\hat{p}) \left(\frac{1}{n} + \frac{1}{m}\right)}}&lt;/script&gt;

&lt;p&gt;and see how extreme it is for a draw from standard normal.&lt;/p&gt;

&lt;p&gt;As an aside, an alternative statistic to use to test significance is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z = \frac{\hat{p}_A - \hat{p}_B}{\sqrt{\hat{p}_A(1-\hat{p}_A) / n + \hat{p}_B(1-\hat{p}_B) / m}}.&lt;/script&gt;

&lt;p&gt;One nice feature of this statistic is that the test for being significant at level &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; using it is equivalent to the &lt;script type=&quot;math/tex&quot;&gt;(1-\alpha)&lt;/script&gt; confidence interval for the difference &lt;script type=&quot;math/tex&quot;&gt;\hat{p}_A - \hat{p}_B&lt;/script&gt; containing 0.&lt;/p&gt;

&lt;p&gt;How can we find a confidence interval for the risk ratio?  The distribution of the ratio of two independent normals is complicated (unless both normals have zero mean, in which case the ratio is distributed Cauchy).  The trick is to turn the ratio into a difference by taking a log, use propagation of error, and then transform back.&lt;/p&gt;

&lt;p&gt;Propagation of error approximately describes how the mean and variance of a random variable change under a transformation.  More precisely, &lt;script type=&quot;math/tex&quot;&gt;\textbf{E}(f(X)) \approx f(\textbf{E}(X))&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\textbf{Var}(f(X)) \approx f'(\textbf{E}(X))^2\ \textbf{Var}(X)&lt;/script&gt;.  Propagation of error is often used in conjunction with the central limit theorem: if &lt;script type=&quot;math/tex&quot;&gt;\sqrt{n}(X_n - \mu)&lt;/script&gt; converges in distribution to &lt;script type=&quot;math/tex&quot;&gt;N(0, \sigma^2)&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;\sqrt{n}(f(X_n) - f(\mu))&lt;/script&gt; converges to &lt;script type=&quot;math/tex&quot;&gt;N(0, f'(\mu)^2 \sigma^2)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;By propagation of error, &lt;script type=&quot;math/tex&quot;&gt;\log \hat{p}_A&lt;/script&gt; is approximately distributed &lt;script type=&quot;math/tex&quot;&gt;N(\log(p_A), \frac{1-p_A}{n p_A})&lt;/script&gt;.  The standard error for the difference in log hat probabilities is thus&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;SE = \sqrt{\frac{1-\hat{p}_A}{n \hat{p}_A} +\frac{1-\hat{p}_B}{m\hat{p}_B}},&lt;/script&gt;

&lt;p&gt;and the &lt;script type=&quot;math/tex&quot;&gt;95\%&lt;/script&gt; confidence interval for the difference is &lt;script type=&quot;math/tex&quot;&gt;\pm 1.96\ SE&lt;/script&gt;.  We exponentiate to get the confidence interval for the risk ratio:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\hat{p}_A}{\hat{p}_B}\ e^{\pm 1.96\ SE}.&lt;/script&gt;

&lt;p&gt;Note the confidence interval is asymmetric!&lt;/p&gt;</content><author><name>Scott Roy</name></author><category term="confidence interval" /><category term="difference in proportions" /><category term="propagation of error" /><category term="ratio of proportions" /><category term="risk ratio" /><summary type="html">Suppose I conduct an experiment to determine whether to use font A or font B in an online ad.  After running the experiment, I find that there is a 1% chance that a user clicks on the font A ad, and a 0.8% chance that the user clicks on the font B ad.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/payperclick.jpg" /></entry></feed>