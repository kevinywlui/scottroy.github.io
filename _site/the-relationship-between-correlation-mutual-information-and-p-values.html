<!doctype html>
<html>

<head>

  <title>
    
      The relationship between correlation, mutual information, and p-values | statsandstuff
    
  </title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">
  <!-- Use Atom -->
    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="statsandstuff" />
  <!-- Use RSS-2.0 -->
  <!--<link href="/rss-feed.xml" type="application/rss+xml" rel="alternate" title="statsandstuff | a blog on statistics and machine learning"/>
  //-->

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quattrocento+Sans">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-135466463-1', 'auto');
  ga('send', 'pageview');
</script>


<meta name="author" content="Scott Roy"/>  
<meta property="og:locale" content="en_US">
<meta property="og:description" content="Feature selection is often necessary before building a machine learning or statistical model, especially when there are many, many irrelevant features. To be more concrete, suppose we want to predict/explain...">
<meta property="description" content="Feature selection is often necessary before building a machine learning or statistical model, especially when there are many, many irrelevant features. To be more concrete, suppose we want to predict/explain...">
<meta property="og:title" content="The relationship between correlation, mutual information, and p-values">
<meta property="og:site_name" content="statsandstuff">
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:4000/the-relationship-between-correlation-mutual-information-and-p-values.html">
<meta property="og:image:url" content="http://localhost:4000/assets/img/correlation_pval_mutual_info.png">
<meta property="og:image:secure_url" content="http://localhost:4000/assets/img/correlation_pval_mutual_info.png">
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="630" />



</head>


<body>

  <div class="container">
    <header class="masthead">
  <h3 class="masthead-title">
    <a href="/">statsandstuff</a>
    <small class="masthead-subtitle">a blog on statistics and machine learning</small>
    <div class="menu">
  <nav class="menu-content">
    
      <a href="/menu/about.html">About</a>
    
      <a href="/menu/writing.html">Writing</a>
    
      <a href="/menu/contact.html">Contact</a>
    
  </nav>
  <nav class="social-icons">
    
  
  
    <a href="https://www.github.com/scottroy" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://www.linkedin.com/in/scott-roy/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  

  
  
    <a href="mailto:scott.michael.roy@gmail.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  

  
  
    <a href="/feed.xml"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  

  </nav>
</div>

  </h3>
</header>


    <div class="post-container">
      <h1>
  The relationship between correlation, mutual information, and p-values
</h1>


  <img src="/assets/img/correlation_pval_mutual_info.png">


<p>Feature selection is often necessary before building a machine learning or statistical model, especially when there are many, many irrelevant features.  To be more concrete, suppose we want to predict/explain some response <script type="math/tex">Y</script> using some features <script type="math/tex">X_1, \ldots, X_k</script>.  A natural first step is to find the features that are “most related” to the response and build a model with those.
There are many ways we could interpret “most related”:</p>

<ul>
  <li>The features most correlated with the response</li>
  <li>The features with the highest mutual information with the response</li>
  <li>The features that are the most “statistically significant” in explaining the response</li>
</ul>

<p>In this post, I want to discuss why any of the above approaches should work well.  The basic insight is that:</p>

<ul>
  <li>The correlation is a reparametrization of p-values obtained via t-tests, F-tests, proportion tests, and chi-squared tests, meaning that ranking features by p-value is equivalent to ranking them by correlation (for fixed sample size <script type="math/tex">N</script>)</li>
  <li>The mutual information is a reparametrization of the p-values obtained by a G-test.  Moreover, the chi-squared statistic is a second order Taylor approximation of the G statistic, and so the ranking by mutual information and correlation is often similar in practice.</li>
</ul>

<p>The post is organized into three scenarios:</p>

<ul>
  <li>Both the response and feature are binary</li>
  <li>Either the response or feature is binary</li>
  <li>Both the response and feature is real-valued</li>
</ul>

<h2 id="both-variables-are-binary">Both variables are binary</h2>

<p>In this section, we assume both the feature <script type="math/tex">X \in \{0,1\}^N</script> and response <script type="math/tex">Y \in \{0, 1\}^N</script> are binary.  We focus on one feature to highlight the relation between the chi-squared test, the correlation, the G-test, and mutual information.
We can summarize the relation between binary variables in a contingency table:</p>

<p><img src="/assets/img/contingency22.png" alt="" /></p>

<p>In the table <script type="math/tex">O_{ij}</script> denotes the number of observations where <script type="math/tex">X = i</script> and  <script type="math/tex">Y = j</script>.  In addition, we let <script type="math/tex">\cdot</script> denote summation over an index; so <script type="math/tex">O_{i \cdot}</script> is the sum of the <script type="math/tex">i</script>th row and <script type="math/tex">O_{\cdot j}</script> is the sum of the <script type="math/tex">j</script>th column. </p>

<h3 id="correlation-and-the-chi-squared-test">Correlation and the chi-squared test</h3>
<p>In the context of binary variables, the Pearson correlation is often called the “phi coefficient” and can be computed from the contingency table itself:</p>

<script type="math/tex; mode=display">\phi = \frac{O_{00} O_{11} - O_{01} O_{10}}{\sqrt{O_{0\cdot} O_{\cdot 0} O_{1 \cdot} O_{\cdot 1}}}.</script>

<p>The phi coefficient is a measure of association between <script type="math/tex">X</script> and <script type="math/tex">Y</script>; it is a product of counts where <script type="math/tex">X</script> and <script type="math/tex">Y</script> agree minus a product of counts where they disagree, normalized by row and column sums so that the value is between -1 and 1.</p>

<p>Another common way to measure the association between two binary variables is the chi-squared test of independence, introduced by Karl Pearson in 1900.  As a reminder, the chi-squared test statistic is</p>

<script type="math/tex; mode=display">\chi^2 = \sum \frac{(O_{ij} - E_{ij})^2}{E_{ij}},</script>

<p>where</p>

<script type="math/tex; mode=display">E_{ij} = N \left( \frac{O_{i \cdot}}{N} \right) \left( \frac{O_{\cdot j}}{N} \right) := N r_i c_j</script>

<p>is the expected number observations in cell <script type="math/tex">(i,j)</script> under the assumption that <script type="math/tex">X</script> and <script type="math/tex">Y</script> are independent.</p>

<p>For fixed sample size <script type="math/tex">N</script>, the phi coefficient is just a reparametrization of the the chi-squared statistic:</p>

<script type="math/tex; mode=display">\phi = \sqrt{\frac{\chi^2}{n}}.</script>

<p>This is easy to show by expanding the chi-squared statistic.  (For those who want to work out the algebra, the following relation is useful:</p>

<script type="math/tex; mode=display">O_{i \cdot} O_{\cdot j} = n O_{ij} + s_{ij} \Delta,</script>

<p>where <script type="math/tex">s_{ij}</script> is either 1 (if <script type="math/tex">i \neq j</script> ) or -1 (<script type="math/tex">i = j</script> ) and <script type="math/tex">\Delta = O_{00} O_{11} - O_{01} O_{10}</script> is the determinant of the contingency table.)</p>

<p>Tying this to the theme of the post, suppose we have a binary response <script type="math/tex">Y</script> and binary features <script type="math/tex">X_1, \ldots, X_k</script>.  Ranking the features by p-value from a chi-squared test with the response is equivalent to ranking the features by absolute correlation with the response.  For fixed sample size <script type="math/tex">N</script>, the p-value itself is a measure of association strength.</p>

<h3 id="difference-in-proportions-test">Difference in proportions test</h3>

<p>When both <script type="math/tex">X</script> and <script type="math/tex">Y</script> are binary, we can view <script type="math/tex">X</script> as defining group membership and <script type="math/tex">Y</script> as defining an outcome.  For example, suppose <script type="math/tex">X</script> indicates whether someone smokes and <script type="math/tex">Y</script> indicates if they have lung cancer.
In this case, the association between <script type="math/tex">X</script> and <script type="math/tex">Y</script> is captured in the difference in the proportion <script type="math/tex">p_1</script> of smokers who get lung cancer and the proportion <script type="math/tex">p_0</script> of non-smokers who do.  The difference in proportions test statistic</p>

<script type="math/tex; mode=display">T = \frac{p_1 - p_0}{\sqrt{p(1-p) \left( \frac{1}{N_1} + \frac{1}{N_0} \right)}}</script>

<p>tests if <script type="math/tex">p_1</script> is different than <script type="math/tex">p_0</script> and is approximately distributed standard normal under the null hypothesis <script type="math/tex">H_0 : p_0 = p_1</script>.</p>

<p>The statistic <script type="math/tex">T</script> is just the square root of the chi-squared test statistic <script type="math/tex">\chi^2</script> and so the two tests are equivalent.  (An easy way to see this is to show that <script type="math/tex">T = \sqrt{N} \phi</script> by writing <script type="math/tex">p_0</script>, <script type="math/tex">p_1</script>, <script type="math/tex">p</script>, <script type="math/tex">N_0</script>, and <script type="math/tex">N_1</script> in terms of the cells <script type="math/tex">O_{ij}</script> of the contingency table.)</p>

<h3 id="mutual-information-and-the-g-test">Mutual information and the G-test</h3>

<p>The likelihood ratio test (LRT) is an alternative to the chi-squared test of independence.  The resulting test statistic is the so-called G-statistic:</p>

<script type="math/tex; mode=display">G = 2 \sum O_{ij} \log \left( \frac{O_{ij}}{E_{ij}} \right).</script>

<p>The relation</p>

<script type="math/tex; mode=display">G = 2 N \ \text{MI}(X, Y)</script>

<p>between <script type="math/tex">G</script> and the mutual information between <script type="math/tex">X</script> 
and <script type="math/tex">Y</script> is immediate (<script type="math/tex">\text{MI}(X, Y)</script> is the Kullback-Leibler divergence of the product of the marginal distributions from the joint distribution).  It follows that the ranking among features induced by mutual information with the response is the same as the ranking induced by p-values computed via a G-test.</p>

<p>In practice this is often similar to the rankings induced by correlation/proportion tests/chi-squared tests because the chi-squared test statistic is the second-order Taylor approximation of the G-statistic (expand the log term about 1).</p>

<p><img src="/assets/img/mutual_info_vs_corr.png" alt="" /></p>

<h2 id="one-variable-is-binary">One variable is binary</h2>

<p>In this section, we assume the feature <script type="math/tex">X \in \{0,1\}^N</script> is a binary vector and the response <script type="math/tex">Y \in \mathbb{R}^N</script> is real-valued.  (We could instead assume <script type="math/tex">Y</script> is binary and <script type="math/tex">X</script> is real-valued.)  As with the difference in proportions test, we can view <script type="math/tex">X</script> as defining two groups (e.g., a treatment and control group in an experiment) and <script type="math/tex">Y</script> as defining some continuous outcome.  A measure of association between <script type="math/tex">X</script> and <script type="math/tex">Y</script> is captured in the difference between the mean outcome <script type="math/tex">\bar{Y}_1</script> in treatment and the mean outcome in control <script type="math/tex">\bar{Y}_0</script>.  This difference is often assessed with a two-sample t-test using the test statistic</p>

<script type="math/tex; mode=display">T = \frac{\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot}}{\sqrt{S_p^2 \left(\frac{1}{N_1} + \frac{1}{N_0} \right)}}.</script>

<p>Here <script type="math/tex">S^2_p</script> denotes the pooled sampled variance</p>

<script type="math/tex; mode=display">S^2_p = \frac{\sum_{i=0}^1 \sum_{j=1}^{N_j} (Y_{ij} - \bar{Y}_{i\cdot})^2}{N-2}.</script>

<p>As with the chi-squared/difference in proportions tests before, the t-statistic <script type="math/tex">T</script> is a reparametrization of the Pearson sample correlation <script type="math/tex">r</script>:</p>

<script type="math/tex; mode=display">r = \frac{T}{\sqrt{N-2 + T^2}}.</script>

<p>Before we walk through the derivation, we define some notation.  The vector <script type="math/tex">X</script> splits the observations <script type="math/tex">Y</script> into two groups: <script type="math/tex">\{ Y_i : X_i = 0 \}</script> and <script type="math/tex">\{ Y_i : X_i = 1 \}</script> .  Let <script type="math/tex">N_0</script> and <script type="math/tex">N_1</script> be the respective sizes of these groups.  We reindex the observations <script type="math/tex">Y</script> using notation from ANOVA.  We let <script type="math/tex">Y_{ij}</script> denote the <script type="math/tex">j</script> th observation (<script type="math/tex">j = 1\ldots N_j</script> ) from the <script type="math/tex">i</script> th group (<script type="math/tex">i = 0, 1</script> ).  The notation <script type="math/tex">\bar{Y}_{i\cdot}</script> denotes the mean of <script type="math/tex">Y</script> over the <script type="math/tex">i</script> th group and <script type="math/tex">\bar{Y}_{\cdot \cdot}</script> denotes the overall mean of <script type="math/tex">Y</script>.</p>

<p>We can write</p>

<script type="math/tex; mode=display">r = \frac{\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot}}{\sqrt{(N-1) S^2 \left(\frac{1}{N_1} + \frac{1}{N_0} \right)}},</script>

<p>where <script type="math/tex">S^2 = \frac{1}{n-1} \sum_{i=1}^N (Y_i - \bar{Y})^2</script> is the sample variance.  This resembles the two sample t-statistic (which hints at the connection), but has the sample variance <script type="math/tex">S^2</script> instead of the pooled variance <script type="math/tex">S^2_p</script>.</p>

<p>We relate <script type="math/tex">S^2</script> and <script type="math/tex">S^2_p</script> with the following sum of squares partition (derivation in the appendix):</p>

<script type="math/tex; mode=display">(N-1) S^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right) = (N-2) S_p^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right) + (\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot})^2.</script>

<p>Using this partition to rewrite the denominator in the correlation expression and dividing numerator and denominator by <script type="math/tex">\sqrt{S_p^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right)}</script> yields</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} r &= \frac{\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot}}{\sqrt{(N-1) S^2 \left(\frac{1}{N_1} + \frac{1}{N_0} \right)}} \\ &= \frac{(\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot}) / \sqrt{S_p^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right)}}{\sqrt{N-2 + (\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot})^2 / \left(S_p^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right)\right)}} \\ &= \frac{T}{\sqrt{N-2 + T^2}}. \end{aligned} %]]></script>

<h2 id="both-variables-are-real-valued">Both variables are real-valued</h2>

<p>Suppose we regress <script type="math/tex">Y \sim 1 + X</script> and get slope <script type="math/tex">\hat{\beta}</script>.
We can use a t-test to test if the slope is different than 0.  The p-value we get is just a reparametrization of correlation.</p>

<p>To make matters simple, let <script type="math/tex">SXX = \sum_{i=1}^N (X_i - \bar{X})^2</script> be the sum of squares for <script type="math/tex">X</script> (and similarly for <script type="math/tex">SYY</script> ), <script type="math/tex">SXY = \sum_{i=1}^N (X_i - \bar{X}) (Y_i - \bar{Y})</script> , and <script type="math/tex">RSS = \sum_{i=1}^N (Y_i - \hat{Y}_i)^2</script> be the residual sum of squares.</p>

<p>We can write the slope <script type="math/tex">\hat{\beta} = \frac{SXY}{SXX}</script> and the correlation between <script type="math/tex">X</script> and <script type="math/tex">Y</script> as <script type="math/tex">r = \hat{\beta} \sqrt{\frac{SXX}{SYY}}.</script></p>

<p>To test whether <script type="math/tex">\hat{\beta}</script> is nonzero, we see how many standard errors it is from 0.  The standard error of <script type="math/tex">\hat{\beta}</script> is <script type="math/tex">\text{se}(\hat{\beta}) = \sqrt{\frac{RSS}{(N-2) SXX}}</script> and the test statistic is</p>

<script type="math/tex; mode=display">T = \frac{\hat{\beta}}{\text{se}(\hat{\beta})} = \hat{\beta} \sqrt{\frac{SXX}{RSS / (N-2)}}.</script>

<p>This reduces to the two-sample t-statistic when <script type="math/tex">X</script> is binary and follows a t-distribution with <script type="math/tex">N-2</script> degrees of freedom.</p>

<p>Dividing numerator and denominator in the expression for <script type="math/tex">r</script> by <script type="math/tex">\sqrt{RSS / (N-2)}</script> (after rewriting <script type="math/tex">SYY = RSS + \hat{\beta}^2 SXX</script> ) we get</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} r &= \hat{\beta} \sqrt{\frac{SXX}{SYY}} \\ &= \frac{\hat{\beta} \sqrt{\frac{SXX}{RSS / (N-2)}}}{\sqrt{N-2 + \hat{\beta}^2 \frac{SXX}{RSS / (N-2)} }} \\ &= \frac{T}{\sqrt{N-2 + T^2}}.  \end{aligned} %]]></script>

<h2 id="summary">Summary</h2>

<p>In this post, we discussed various ways of measuring association between a response <script type="math/tex">Y</script> and predictors <script type="math/tex">X_1, \ldots, X_p</script> in the context of feature selection.  We showed that all the methods are more or less equivalent, which we summarize in the following diagram.</p>

<p><img src="/assets/img/correlation_pval_mutual_info.png" alt="" /></p>

<p>A solid connector indicates the two quantities are reparametrizations of each other (i.e., there is an increasing function that maps one to the other).  The dashed line between the G-statistic and the chi-squared statistic indicates that these quantities are approximately equivalent and so give similar rankings in practice.</p>

<h2 id="appendix-partitioning-the-sum-of-squares">Appendix: partitioning the sum of squares</h2>
<p>Partitioning the variation is fundamental in ANOVA and regression analysis and is a simple consequence of the Pythagorean theorem.  Define the following three vectors</p>

<script type="math/tex; mode=display">Y = \begin{bmatrix} \begin{pmatrix} Y_{11} \\ Y_{12} \\ \vdots \\ Y_{1 N_1} \end{pmatrix} \\ \begin{pmatrix} Y_{21} \\ Y_{22} \\ \vdots \\ Y_{2 N_2} \end{pmatrix} \end{bmatrix} \quad Y_{\text{trt}} = \begin{bmatrix} \begin{pmatrix} \bar{Y}_{1 \cdot} \\ \bar{Y}_{1 \cdot} \\ \vdots \\ \bar{Y}_{1 \cdot} \end{pmatrix} \\ \begin{pmatrix} \bar{Y}_{2 \cdot} \\ \bar{Y}_{2 \cdot} \\ \vdots \\ \bar{Y}_{2 \cdot} \end{pmatrix} \end{bmatrix} \quad  \bar{Y}_{\cdot \cdot} = \begin{bmatrix} \begin{pmatrix} \bar{Y}_{\cdot \cdot} \\ \bar{Y}_{\cdot \cdot} \\ \vdots \\ \bar{Y}_{\cdot \cdot} \end{pmatrix} \\ \begin{pmatrix} \bar{Y}_{\cdot \cdot} \\ \bar{Y}_{\cdot \cdot} \\ \vdots \\ \bar{Y}_{\cdot \cdot} \end{pmatrix} \end{bmatrix}</script>

<p>The vectors <script type="math/tex">(Y_{\text{trt}} - \bar{Y}_{\cdot \cdot})</script> and <script type="math/tex">(Y - Y_{\text{trt}})</script> are orthogonal.  Applying the Pythagorean theorem to the decomposition <script type="math/tex">Y - \bar{Y}_{\cdot \cdot} = (Y - Y_{\text{trt}}) + (Y_{\text{trt}} - \bar{Y}_{\cdot \cdot})</script> gives the sum of squares decomposition used above.</p>

<h2 id="tangent-r2-and-f-tests">Tangent: <script type="math/tex">R^2</script> and F tests</h2>
<p>In this section I discuss the relationship between the F statistic and <script type="math/tex">R^2</script>, the coefficient of determination.
The F statistic is a generalization of the t-test for an OLS slope, but does not fit into the “feature selection” narrative of the post.</p>

<p>The <script type="math/tex">R^2</script> is the fraction of variance explained by a linear model</p>

<script type="math/tex; mode=display">R^2 = \frac{\text{SS}_{\text{reg}}}{SYY} = \frac{\text{SYY} - \text{RSS}}{\text{SYY}} = 1 - \frac{\text{RSS}}{\text{SYY}}.</script>

<p>The F statistic to test the fit of a multivariate linear model (compared to a simple intercept model) is:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
F &= \frac{\text{SS}_{\text{reg}} / k}{\text{RSS} / (n-k-1)} \\
&= \frac{(\text{SYY} - \text{RSS}) / k}{\text{RSS} / (n-k-1)} \\
&= \left( \frac{n-k-1}{k} \right) \left( \frac{\text{SYY}}{\text{RSS}} - 1 \right).
\end{aligned} %]]></script>

<p>(See <a href="/geometric-interpretations-of-linear-regression-and-ANOVA.html">Geometric interpretations of linear regression and ANOVA</a> for a discussion of the F statistic.)</p>

<p>We can write the F statistic as an increasing function of <script type="math/tex">R^2 = 1 - \frac{\text{RSS}}{\text{SYY}}</script>:</p>

<script type="math/tex; mode=display">F = \left( \frac{n-k-1}{k} \right) \left( \frac{R^2}{1-R^2} \right).</script>

<p>The <script type="math/tex">R^2</script> is expressable as the square correlation between predicted and observed values:</p>

<script type="math/tex; mode=display">R^2 = \text{corr}\left(Y, \hat{Y} \right)^2.</script>

<p>It follows that an F-statistic p-value of a multivariate regression model is an increasing function of the absolute correlation between the observed values <script type="math/tex">Y</script> and the model’s predicted values <script type="math/tex">\hat{Y}</script>.</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Phi_coefficient">Phi coefficient</a></li>
  <li><a href="https://en.wikipedia.org/wiki/G-test">G-test</a></li>
  <li><em>Applied Linear Regression</em> by Sanford Weisberg</li>
</ul>


<span class="post-date">
  Written on
  
  March
  3rd
    ,
  2019
  by
  
    Scott Roy
  
</span>

<div class="post-date">Feel free to share!</div>
  <div class="sharing-icons">
    <a href="https://twitter.com/intent/tweet?text=The relationship between correlation, mutual information, and p-values&amp;url=/the-relationship-between-correlation-mutual-information-and-p-values.html" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
    <a href="https://www.facebook.com/sharer/sharer.php?u=/the-relationship-between-correlation-mutual-information-and-p-values.html&amp;title=The relationship between correlation, mutual information, and p-values" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
    <a href="https://plus.google.com/share?url=/the-relationship-between-correlation-mutual-information-and-p-values.html" target="_blank"><i class="fa fa-google-plus" aria-hidden="true"></i></a>
  </div>
</div>


<div class="related">
  <h1 >You may also enjoy:</h1>
  
  <ul class="related-posts">
    
      
        
        
      
    
      
        
        
      
    
      
        
        
      
    
      
        
        
      
    
      
        
        
      
    
      
        
        
      
    
      
        
        
      
    
      
        
        
      
    
      
        
        
      
    
  </ul>
</div>




    </div>

    <footer class="footer">
  
  
  
    <a href="https://www.github.com/scottroy" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://www.linkedin.com/in/scott-roy/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  

  
  
    <a href="mailto:scott.michael.roy@gmail.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  

  
  
    <a href="/feed.xml"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  

  <div class="post-date"><a href="/menu/about.html">statsandstuff | a blog on statistics and machine learning by Scott Roy</a></div>
</footer>

  </div>

</body>
</html>
